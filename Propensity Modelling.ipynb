{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95962df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from src.utils.config import get_small_classifier_config, get_medium_classifier_config, get_large_classifier_config\n",
    "from src.training.classifier_trainer import SimpleTextDataset, train_classifier, evaluate, evaluate_from_dataframe\n",
    "import csv, random, time, datetime as dt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.utils.char_tokenizer import CharTokenizer\n",
    "from src.training.data_loader import create_data_loader\n",
    "from torch.utils.data import DataLoader\n",
    "from src.utils.tokenizer import SimpleTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b81458",
   "metadata": {},
   "source": [
    "### Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f024419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "cfg = get_medium_classifier_config()\n",
    "cfg.num_classes = 2  # binary\n",
    "\n",
    "# Adjust hyper parameters\n",
    "# cfg.learning_rate = 1e-4\n",
    "# cfg.weight_decay = 0.01\n",
    "# cfg.max_epochs = 8\n",
    "# cfg.temperature = 0.1\n",
    "cfg.max_new_tokens = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d973f902",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43e69206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_pseudo_id</th>\n",
       "      <th>sequence_start_monday</th>\n",
       "      <th>day_num</th>\n",
       "      <th>day</th>\n",
       "      <th>total_session_starts</th>\n",
       "      <th>total_page_views</th>\n",
       "      <th>total_button_click</th>\n",
       "      <th>total_add_to_cart</th>\n",
       "      <th>total_begin_checkout</th>\n",
       "      <th>total_view_item</th>\n",
       "      <th>total_view_item_list</th>\n",
       "      <th>total_view_promotion</th>\n",
       "      <th>total_select_promotion</th>\n",
       "      <th>total_remove_from_cart</th>\n",
       "      <th>total_purchase_events</th>\n",
       "      <th>total_purchase_revenue</th>\n",
       "      <th>total_unique_items</th>\n",
       "      <th>total_item_quantity</th>\n",
       "      <th>purchases_next_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000012e+09</td>\n",
       "      <td>2023-07-24</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000012e+09</td>\n",
       "      <td>2023-07-24</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-07-25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000012e+09</td>\n",
       "      <td>2023-07-24</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-07-26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000012e+09</td>\n",
       "      <td>2023-07-24</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-07-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000012e+09</td>\n",
       "      <td>2023-07-24</td>\n",
       "      <td>5</td>\n",
       "      <td>2023-07-28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_pseudo_id sequence_start_monday  day_num         day  \\\n",
       "0    1.000012e+09            2023-07-24        1  2023-07-24   \n",
       "1    1.000012e+09            2023-07-24        2  2023-07-25   \n",
       "2    1.000012e+09            2023-07-24        3  2023-07-26   \n",
       "3    1.000012e+09            2023-07-24        4  2023-07-27   \n",
       "4    1.000012e+09            2023-07-24        5  2023-07-28   \n",
       "\n",
       "   total_session_starts  total_page_views  total_button_click  \\\n",
       "0                     0                 0                   0   \n",
       "1                     0                 0                   0   \n",
       "2                     0                 0                   0   \n",
       "3                     0                 0                   0   \n",
       "4                     0                 0                   0   \n",
       "\n",
       "   total_add_to_cart  total_begin_checkout  total_view_item  \\\n",
       "0                  0                     0                0   \n",
       "1                  0                     0                0   \n",
       "2                  0                     0                0   \n",
       "3                  0                     0                0   \n",
       "4                  0                     0                0   \n",
       "\n",
       "   total_view_item_list  total_view_promotion  total_select_promotion  \\\n",
       "0                     0                     0                       0   \n",
       "1                     0                     0                       0   \n",
       "2                     0                     0                       0   \n",
       "3                     0                     0                       0   \n",
       "4                     0                     0                       0   \n",
       "\n",
       "   total_remove_from_cart  total_purchase_events  total_purchase_revenue  \\\n",
       "0                       0                      0                     0.0   \n",
       "1                       0                      0                     0.0   \n",
       "2                       0                      0                     0.0   \n",
       "3                       0                      0                     0.0   \n",
       "4                       0                      0                     0.0   \n",
       "\n",
       "   total_unique_items  total_item_quantity purchases_next_week  \n",
       "0                   0                    0                   N  \n",
       "1                   0                    0                   N  \n",
       "2                   0                    0                   N  \n",
       "3                   0                    0                   N  \n",
       "4                   0                    0                   N  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_pseudo_id            float64\n",
      "sequence_start_monday      object\n",
      "day_num                     int64\n",
      "day                        object\n",
      "total_session_starts        int64\n",
      "total_page_views            int64\n",
      "total_button_click          int64\n",
      "total_add_to_cart           int64\n",
      "total_begin_checkout        int64\n",
      "total_view_item             int64\n",
      "total_view_item_list        int64\n",
      "total_view_promotion        int64\n",
      "total_select_promotion      int64\n",
      "total_remove_from_cart      int64\n",
      "total_purchase_events       int64\n",
      "total_purchase_revenue    float64\n",
      "total_unique_items          int64\n",
      "total_item_quantity         int64\n",
      "purchases_next_week        object\n",
      "dtype: object\n",
      "Rows: 274400\n"
     ]
    }
   ],
   "source": [
    "csv_path = Path(\"Propensity Modelling Data V4 Very Large.csv\")  # adjust if stored elsewhere\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# ensure numeric types\n",
    "# df[\"rev_usd\"] = df[\"rev_usd\"].astype(float)\n",
    "# df[\"event_timestamp\"] = df[\"event_timestamp\"].astype(\"int64\")\n",
    "\n",
    "display(df.head())\n",
    "print(df.dtypes)\n",
    "print(f\"Rows: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e8e944",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = df.copy()\n",
    "\n",
    "# Convert sequence_start_monday to date time\n",
    "processed_data = processed_data.dropna(subset=[\"sequence_start_monday\"])\n",
    "processed_data[\"day\"] = pd.to_datetime(processed_data[\"day\"])\n",
    "\n",
    "# Convert str to int\n",
    "processed_data[\"total_session_starts\"] = processed_data[\"total_session_starts\"].fillna(0).astype(int)\n",
    "processed_data[\"total_page_views\"] = processed_data[\"total_page_views\"].fillna(0).astype(int)\n",
    "processed_data[\"total_button_click\"] = processed_data[\"total_button_click\"].fillna(0).astype(int)\n",
    "processed_data[\"total_add_to_cart\"] = processed_data[\"total_add_to_cart\"].fillna(0).astype(int)\n",
    "processed_data[\"total_begin_checkout\"] = processed_data[\"total_begin_checkout\"].fillna(0).astype(int)\n",
    "processed_data[\"total_view_item\"] = processed_data[\"total_view_item\"].fillna(0).astype(int)\n",
    "processed_data[\"total_view_item_list\"] = processed_data[\"total_view_item_list\"].fillna(0).astype(int)\n",
    "processed_data[\"total_view_promotion\"] = processed_data[\"total_view_promotion\"].fillna(0).astype(int)\n",
    "processed_data[\"total_select_promotion\"] = processed_data[\"total_select_promotion\"].fillna(0).astype(int)\n",
    "processed_data[\"total_remove_from_cart\"] = processed_data[\"total_remove_from_cart\"].fillna(0).astype(int)\n",
    "processed_data[\"total_purchase_events\"] = processed_data[\"total_purchase_events\"].fillna(0).astype(int)\n",
    "processed_data[\"total_purchase_revenue\"] = processed_data[\"total_purchase_revenue\"].fillna(0)\n",
    "processed_data[\"total_purchase_revenue\"] = processed_data[\"total_purchase_revenue\"].astype(str).str.replace(',', '').astype(float)\n",
    "processed_data[\"total_unique_items\"] = processed_data[\"total_unique_items\"].fillna(0).astype(int)\n",
    "processed_data[\"total_item_quantity\"] = processed_data[\"total_item_quantity\"].fillna(0).astype(int)\n",
    "\n",
    "# Convert Y/N to 1/0 in purchase event\n",
    "processed_data[\"purchases_next_week\"] = processed_data[\"purchases_next_week\"].map({'Y': 1, 'N': 0})\n",
    "\n",
    "# Configuration for temporal ordering\n",
    "NEWEST_FIRST = True  # Set to True for newest events first, False for oldest events first\n",
    "\n",
    "# grab unique user ids\n",
    "unique_user_ids = df[\"user_pseudo_id\"].unique()\n",
    "train_data = []\n",
    "print(f\"Processing {len(unique_user_ids)} unique users...\")\n",
    "print(f\"Temporal ordering: {'NEWEST â†’ OLDEST' if NEWEST_FIRST else 'OLDEST â†’ NEWEST'}\")\n",
    "\n",
    "for user_id in unique_user_ids:\n",
    "    user_data = processed_data[processed_data[\"user_pseudo_id\"] == user_id]\n",
    "\n",
    "    event_len = len(user_data)\n",
    "    for i in range(event_len-7, event_len):\n",
    "        main_event = user_data.iloc[i]\n",
    "        # Get start of main_week(monday)\n",
    "        main_start_of_week = main_event[\"day\"] - pd.to_timedelta(main_event[\"day\"].dayofweek, unit='d')\n",
    "        main_end_of_week = main_start_of_week + pd.DateOffset(days=6)\n",
    "        pred_start_of_week = main_end_of_week + pd.Timedelta(days=1)\n",
    "        pred_end_of_week = pred_start_of_week + pd.DateOffset(days=6)\n",
    "\n",
    "        context_events = user_data.iloc[:i]\n",
    "        \n",
    "        # Reverse order if we want newest events first\n",
    "        if NEWEST_FIRST:\n",
    "            context_events = context_events.iloc[::-1]  # Reverse the DataFrame\n",
    "        \n",
    "        train_data_record = \"\"\n",
    "        empty_record = True\n",
    "        \n",
    "        for event in context_events.itertuples():\n",
    "            # Check how many days before pred_start_of_week\n",
    "            check_day = (pred_start_of_week - event.day).days\n",
    "            train_data_record_line = \"\"\n",
    "            empty_event = True\n",
    "            if event.total_session_starts > 0:\n",
    "                empty_record = False\n",
    "                empty_event = False\n",
    "                train_data_record_line += f\", ssn_srts: {event.total_session_starts}\"\n",
    "            if event.total_page_views > 0:\n",
    "                empty_record = False\n",
    "                empty_event = False\n",
    "                train_data_record_line += f\", pg_vws: {event.total_page_views}\"\n",
    "            if event.total_button_click > 0:\n",
    "                empty_record = False\n",
    "                empty_event = False\n",
    "                train_data_record_line += f\", btn_clk: {event.total_button_click}\"\n",
    "            if event.total_add_to_cart > 0:\n",
    "                empty_record = False\n",
    "                empty_event = False\n",
    "                train_data_record_line += f\", add_2_crt: {event.total_add_to_cart}\"\n",
    "            if event.total_begin_checkout > 0:\n",
    "                empty_record = False\n",
    "                empty_event = False\n",
    "                train_data_record_line += f\", bgn_chkout: {event.total_begin_checkout}\"\n",
    "            if event.total_view_item > 0:\n",
    "                empty_record = False\n",
    "                empty_event = False\n",
    "                train_data_record_line += f\", vw_itm: {event.total_view_item}\"\n",
    "            if event.total_view_item_list > 0:\n",
    "                empty_record = False\n",
    "                empty_event = False\n",
    "                train_data_record_line += f\", vw_itm_lst: {event.total_view_item_list}\"\n",
    "            if event.total_view_promotion > 0:\n",
    "                empty_record = False\n",
    "                empty_event = False\n",
    "                train_data_record_line += f\", vw_prmtn: {event.total_view_promotion}\"\n",
    "            if event.total_select_promotion > 0:\n",
    "                empty_record = False\n",
    "                empty_event = False\n",
    "                train_data_record_line += f\", slct_prmtn: {event.total_select_promotion}\"\n",
    "            if event.total_remove_from_cart > 0:\n",
    "                empty_record = False\n",
    "                empty_event = False\n",
    "                train_data_record_line += f\", rmv_frm_crt: {event.total_remove_from_cart}\"\n",
    "            if event.total_purchase_events > 0:\n",
    "                empty_record = False\n",
    "                empty_event = False\n",
    "                train_data_record_line += f\", prchs_evts: {event.total_purchase_events}\"\n",
    "            if event.total_purchase_revenue > 0:\n",
    "                empty_record = False\n",
    "                empty_event = False\n",
    "                train_data_record_line += f\", prchs_rev: ${event.total_purchase_revenue}\"\n",
    "            if event.total_unique_items > 0:\n",
    "                empty_record = False\n",
    "                empty_event = False\n",
    "                train_data_record_line += f\", uq_itms: {event.total_unique_items}\"\n",
    "            if event.total_item_quantity > 0:\n",
    "                empty_record = False\n",
    "                empty_event = False\n",
    "                train_data_record_line += f\", itm_qty: {event.total_item_quantity}\"\n",
    "            train_data_record_line += \"\\n\"\n",
    "            if not empty_event:\n",
    "                train_data_record += f\"ds: {check_day}{train_data_record_line}\"\n",
    "\n",
    "        if not empty_record:\n",
    "            train_data.append({\n",
    "                \"text\": train_data_record,\n",
    "                \"label\": main_event[\"purchases_next_week\"]\n",
    "            })\n",
    "\n",
    "tokenizer = SimpleTokenizer()\n",
    "\n",
    "cfg.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "print(f\"Training Data Len: {len(train_data)}\")\n",
    "print(f\"Distribution Balance: {Counter([d['label'] for d in train_data])}\")\n",
    "\n",
    "# Show example of temporal ordering\n",
    "if len(train_data) > 0:\n",
    "    print(f\"\\nðŸ“ Example sequence (showing temporal order):\")\n",
    "    example_lines = train_data[0][\"text\"].split('\\n')[:5]  # First 5 lines\n",
    "    for line in example_lines:\n",
    "        if line.strip():\n",
    "            print(f\"   {line}\")\n",
    "    print(f\"   ... (showing first 5 events)\")\n",
    "    print(f\"Label: {train_data[0]['label']}\")\n",
    "\n",
    "tokenized_texts = []\n",
    "attention_masks = []\n",
    "labels = []\n",
    "for row in train_data:\n",
    "    tokens = tokenizer.encode(\n",
    "        text=row[\"text\"],\n",
    "        max_length=cfg.max_seq_len,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "     # Handle different tokenizer return types\n",
    "    if hasattr(tokens, 'size'):  # PyTorch tensor (GPT-2 tokenizer)\n",
    "        if tokens.size(1) > 1:  # Only keep non-empty sequences\n",
    "            squeezed_tokens = tokens.squeeze(0)\n",
    "            tokenized_texts.append(squeezed_tokens)\n",
    "            attention_masks.append(torch.ones_like(squeezed_tokens))  # Use squeezed tokens for mask\n",
    "            labels.append(row[\"label\"])\n",
    "    elif isinstance(tokens, list):  # List of tokens (char tokenizer)\n",
    "        if len(tokens) > 1:  # Only keep non-empty sequences\n",
    "            tokens_tensor = torch.tensor(tokens, dtype=torch.long)\n",
    "            tokenized_texts.append(tokens_tensor)\n",
    "            attention_masks.append(torch.ones_like(tokens_tensor))\n",
    "            labels.append(row[\"label\"])\n",
    "    else:  # Convert to tensor if needed\n",
    "        tokens_tensor = torch.tensor(tokens, dtype=torch.long)\n",
    "        if len(tokens_tensor) > 1:\n",
    "            tokenized_texts.append(tokens_tensor)\n",
    "            attention_masks.append(torch.ones_like(tokens_tensor))\n",
    "            labels.append(row[\"label\"])\n",
    "\n",
    "train_df = pd.DataFrame({\n",
    "    'input_ids': tokenized_texts,\n",
    "    'attention_mask': attention_masks,\n",
    "    'label': labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634b4a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training df for future use\n",
    "train_df.to_pickle(\"training_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d07e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training df if needed\n",
    "train_df = pd.read_pickle(\"training_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1134e4",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e068f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_enc_df, val_enc_df = train_test_split(train_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc0a66d",
   "metadata": {},
   "source": [
    "### Train and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b8d123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU and Memory Diagnostics\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ” SYSTEM DIAGNOSTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check PyTorch and CUDA setup\n",
    "print(f\"ðŸ“¦ PyTorch version: {torch.__version__}\")\n",
    "print(f\"ðŸ”§ CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ðŸš€ CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"ðŸŽ® GPU device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"ðŸ’¾ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"ðŸ”‹ Current GPU memory usage:\")\n",
    "    print(f\"   Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"   Reserved:  {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # Test GPU tensor creation\n",
    "    try:\n",
    "        test_tensor = torch.randn(100, 100).cuda()\n",
    "        print(\"âœ… GPU tensor creation successful\")\n",
    "        del test_tensor\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ GPU tensor creation failed: {e}\")\n",
    "else:\n",
    "    print(\"âš ï¸  CUDA not available - will use CPU\")\n",
    "\n",
    "# Check dataset memory requirements\n",
    "print(f\"\\nðŸ“Š DATASET INFO:\")\n",
    "print(f\"Total samples: {len(train_df):,}\")\n",
    "print(f\"Training samples: {len(train_enc_df):,}\")\n",
    "print(f\"Validation samples: {len(val_enc_df):,}\")\n",
    "\n",
    "# Estimate memory requirements\n",
    "sample_tensor = train_df['input_ids'].iloc[0]\n",
    "if hasattr(sample_tensor, 'numel'):\n",
    "    avg_seq_len = sample_tensor.numel()\n",
    "else:\n",
    "    avg_seq_len = len(sample_tensor)\n",
    "\n",
    "estimated_mem_per_sample = avg_seq_len * 4 / 1024**2  # 4 bytes per token, convert to MB\n",
    "total_estimated_mem = estimated_mem_per_sample * len(train_df)\n",
    "\n",
    "print(f\"Average sequence length: {avg_seq_len}\")\n",
    "print(f\"Estimated memory per sample: {estimated_mem_per_sample:.2f} MB\")\n",
    "print(f\"Total estimated dataset memory: {total_estimated_mem:.1f} MB\")\n",
    "\n",
    "# Memory recommendations\n",
    "if total_estimated_mem > 1000:  # > 1GB\n",
    "    print(\"âš ï¸  Large dataset detected - consider:\")\n",
    "    print(\"   â€¢ Reducing batch size\")\n",
    "    print(\"   â€¢ Reducing max_seq_len\") \n",
    "    print(\"   â€¢ Using gradient accumulation\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330c1923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test Model with GPU optimization\n",
    "import gc\n",
    "\n",
    "# Check GPU availability and memory\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ðŸš€ CUDA GPU detected: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"ðŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"ðŸ”‹ GPU Memory Available: {torch.cuda.memory_reserved(0) / 1024**3:.1f} GB allocated\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"âš ï¸  No GPU detected, using CPU\")\n",
    "    device = 'cpu'\n",
    "\n",
    "# Clear any existing GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Ensure config uses appropriate batch size for GPU\n",
    "if device == 'cuda':\n",
    "    # Reduce batch size if using large dataset to avoid memory issues\n",
    "    if len(train_df) > 10000:\n",
    "        cfg.batch_size = 4  # Smaller batch size for large datasets\n",
    "        print(f\"ðŸ“‰ Reduced batch size to {cfg.batch_size} for large dataset\")\n",
    "    elif len(train_df) > 5000:\n",
    "        cfg.batch_size = 8\n",
    "        print(f\"ðŸ“‰ Reduced batch size to {cfg.batch_size} for medium dataset\")\n",
    "    else:\n",
    "        cfg.batch_size = 16  # Default for smaller datasets\n",
    "        print(f\"ðŸ“Š Using batch size: {cfg.batch_size}\")\n",
    "else:\n",
    "    cfg.batch_size = 2  # Very small batch size for CPU\n",
    "    print(f\"ðŸŒ Using CPU batch size: {cfg.batch_size}\")\n",
    "\n",
    "print(f\"ðŸŽ¯ Training on device: {device}\")\n",
    "print(f\"ðŸ“Š Dataset size: {len(train_df):,} samples\")\n",
    "print(f\"ðŸ”„ Training samples: {len(train_enc_df):,}\")\n",
    "print(f\"âœ… Validation samples: {len(val_enc_df):,}\")\n",
    "\n",
    "try:\n",
    "    # Train the model with explicit device specification\n",
    "    model = train_classifier(cfg, train_enc_df, val_enc_df, device=device)\n",
    "    \n",
    "    # Save model state\n",
    "    torch.save(model.state_dict(), \"classifier_model.pt\")\n",
    "    print(\"âœ… Model saved successfully to classifier_model.pt\")\n",
    "    \n",
    "    # Clear GPU memory after training\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"ðŸ§¹ GPU memory cleared\")\n",
    "        \n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e) or \"not enough memory\" in str(e):\n",
    "        print(\"âŒ Memory error detected!\")\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        print(\"\\nðŸ”§ TRYING MEMORY OPTIMIZATION...\")\n",
    "        \n",
    "        # Clear memory\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # Try with even smaller batch size\n",
    "        original_batch_size = cfg.batch_size\n",
    "        cfg.batch_size = max(1, cfg.batch_size // 2)\n",
    "        print(f\"ðŸ“‰ Reducing batch size from {original_batch_size} to {cfg.batch_size}\")\n",
    "        \n",
    "        # Try training again\n",
    "        try:\n",
    "            model = train_classifier(cfg, train_enc_df, val_enc_df, device=device)\n",
    "            torch.save(model.state_dict(), \"classifier_model.pt\")\n",
    "            print(\"âœ… Model trained successfully with reduced batch size!\")\n",
    "        except Exception as e2:\n",
    "            print(f\"âŒ Still failing with error: {str(e2)}\")\n",
    "            print(\"ðŸ’¡ Suggestions:\")\n",
    "            print(\"   1. Try reducing max_seq_len in config\")\n",
    "            print(\"   2. Use even smaller batch size\")\n",
    "            print(\"   3. Reduce model size (d_model, n_layers)\")\n",
    "            print(\"   4. Use CPU training with very small batch size\")\n",
    "            \n",
    "            # Force CPU training as last resort\n",
    "            print(\"\\nðŸ”„ ATTEMPTING CPU TRAINING AS FALLBACK...\")\n",
    "            cfg.batch_size = 1\n",
    "            try:\n",
    "                model = train_classifier(cfg, train_enc_df, val_enc_df, device='cpu')\n",
    "                torch.save(model.state_dict(), \"classifier_model.pt\")\n",
    "                print(\"âœ… Model trained successfully on CPU!\")\n",
    "            except Exception as e3:\n",
    "                print(f\"âŒ CPU training also failed: {str(e3)}\")\n",
    "                raise e3\n",
    "    else:\n",
    "        print(f\"âŒ Unexpected error: {str(e)}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2d5a2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Using device: cuda\n",
      "ðŸ”§ Model not in memory, creating new model...\n",
      "âœ… Model created and loaded from saved state on cuda\n",
      "âœ… Model created and loaded from saved state on cuda\n"
     ]
    }
   ],
   "source": [
    "# Load model for evaluation with proper device handling\n",
    "import os\n",
    "\n",
    "# Determine device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"ðŸŽ¯ Using device: {device}\")\n",
    "\n",
    "# Check if model file exists\n",
    "model_path = \"classifier_model.pt\"\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"âŒ Model file '{model_path}' not found!\")\n",
    "    print(\"Please train the model first by running the training cell.\")\n",
    "else:\n",
    "    try:\n",
    "        # First check if model variable exists\n",
    "        if 'model' in locals():\n",
    "            print(\"ðŸ“Š Model already exists in memory\")\n",
    "            # Load state dict with proper device mapping\n",
    "            if device == 'cuda':\n",
    "                state_dict = torch.load(model_path)\n",
    "            else:\n",
    "                # Map CUDA tensors to CPU if needed\n",
    "                state_dict = torch.load(model_path, map_location='cpu')\n",
    "            \n",
    "            model.load_state_dict(state_dict)\n",
    "            model = model.to(device)\n",
    "            print(f\"âœ… Model loaded successfully from saved state on {device}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"ðŸ”§ Model not in memory, creating new model...\")\n",
    "            # Model not defined, need to create it first\n",
    "            from src.training.classifier_trainer import build_model\n",
    "            \n",
    "            # Create model on the correct device\n",
    "            model = build_model(cfg)\n",
    "            \n",
    "            # Load state dict with proper device mapping\n",
    "            if device == 'cuda':\n",
    "                state_dict = torch.load(model_path)\n",
    "            else:\n",
    "                # Map CUDA tensors to CPU if needed\n",
    "                state_dict = torch.load(model_path, map_location='cpu')\n",
    "            \n",
    "            model.load_state_dict(state_dict)\n",
    "            model = model.to(device)\n",
    "            print(f\"âœ… Model created and loaded from saved state on {device}\")\n",
    "            \n",
    "        # Clear any excess memory\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            print(\"âŒ GPU memory error during model loading!\")\n",
    "            print(\"ðŸ”„ Trying to load on CPU...\")\n",
    "            \n",
    "            # Force CPU loading\n",
    "            try:\n",
    "                from src.training.classifier_trainer import build_model\n",
    "                model = build_model(cfg)\n",
    "                state_dict = torch.load(model_path, map_location='cpu')\n",
    "                model.load_state_dict(state_dict)\n",
    "                model = model.to('cpu')\n",
    "                device = 'cpu'  # Update device for future operations\n",
    "                print(\"âœ… Model loaded successfully on CPU\")\n",
    "            except Exception as e2:\n",
    "                print(f\"âŒ Failed to load model on CPU: {e2}\")\n",
    "                raise e2\n",
    "        else:\n",
    "            print(f\"âŒ Unexpected error loading model: {e}\")\n",
    "            raise e\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading model: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48d3577f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Evaluation Results ===\n",
      "\n",
      "ðŸŽ¯ Overall Performance:\n",
      "   Validation Loss: 0.1055\n",
      "   Accuracy: 0.9680 (96.80%)\n",
      "\n",
      "ðŸ“Š Confusion Matrix:\n",
      "                 Predicted\n",
      "              No Purchase  Purchase\n",
      "Actual No       5722        77\n",
      "    Purchase     146      1017\n",
      "\n",
      "ðŸ“ˆ Detailed Classification Metrics:\n",
      "   Class 0 (No Purchase):\n",
      "      Precision: 0.9751\n",
      "      Recall:    0.9867\n",
      "      F1-Score:  0.9809\n",
      "      Support:   5799\n",
      "\n",
      "   Class 1 (Purchase):\n",
      "      Precision: 0.9296\n",
      "      Recall:    0.8745\n",
      "      F1-Score:  0.9012\n",
      "      Support:   1163\n",
      "\n",
      "   ðŸ“‹ Summary Metrics:\n",
      "      Macro Avg F1:    0.9410\n",
      "      Weighted Avg F1: 0.9676\n"
     ]
    }
   ],
   "source": [
    "# Test evaluation\n",
    "results = evaluate_from_dataframe(model, val_enc_df, 'cuda' if torch.cuda.is_available() else 'cpu', return_metrics=True)\n",
    "print(\"=== Model Evaluation Results ===\")\n",
    "print()\n",
    "\n",
    "# Unpack the results tuple\n",
    "val_loss, accuracy, confusion_matrix, classification_report = results\n",
    "\n",
    "print(f\"ðŸŽ¯ Overall Performance:\")\n",
    "print(f\"   Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"   Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print()\n",
    "\n",
    "print(f\"ðŸ“Š Confusion Matrix:\")\n",
    "print(f\"                 Predicted\")\n",
    "print(f\"              No Purchase  Purchase\")\n",
    "print(f\"Actual No     {confusion_matrix[0,0]:>6}    {confusion_matrix[0,1]:>6}\")\n",
    "print(f\"    Purchase  {confusion_matrix[1,0]:>6}    {confusion_matrix[1,1]:>6}\")\n",
    "print()\n",
    "\n",
    "print(f\"ðŸ“ˆ Detailed Classification Metrics:\")\n",
    "# Handle different types of classification_report (dict vs string)\n",
    "if isinstance(classification_report, dict):\n",
    "\tprint(f\"   Class 0 (No Purchase):\")\n",
    "\tprint(f\"      Precision: {classification_report.get('0', {}).get('precision', 0.0):.4f}\")\n",
    "\tprint(f\"      Recall:    {classification_report.get('0', {}).get('recall', 0.0):.4f}\")\n",
    "\tprint(f\"      F1-Score:  {classification_report.get('0', {}).get('f1-score', 0.0):.4f}\")\n",
    "\tprint(f\"      Support:   {int(classification_report.get('0', {}).get('support', 0))}\")\n",
    "\tprint()\n",
    "\tprint(f\"   Class 1 (Purchase):\")\n",
    "\tprint(f\"      Precision: {classification_report.get('1', {}).get('precision', 0.0):.4f}\")\n",
    "\tprint(f\"      Recall:    {classification_report.get('1', {}).get('recall', 0.0):.4f}\")\n",
    "\tprint(f\"      F1-Score:  {classification_report.get('1', {}).get('f1-score', 0.0):.4f}\")\n",
    "\tprint(f\"      Support:   {int(classification_report.get('1', {}).get('support', 0))}\")\n",
    "\tprint()\n",
    "\tprint(f\"   ðŸ“‹ Summary Metrics:\")\n",
    "\tprint(f\"      Macro Avg F1:    {classification_report.get('macro avg', {}).get('f1-score', 0.0):.4f}\")\n",
    "\tprint(f\"      Weighted Avg F1: {classification_report.get('weighted avg', {}).get('f1-score', 0.0):.4f}\")\n",
    "else:\n",
    "\tprint(f\"   Classification Report:\")\n",
    "\tprint(classification_report)\n",
    "\n",
    "# Eval Results: Small Model\n",
    "# === Model Evaluation Results ===\n",
    "\n",
    "# ðŸŽ¯ Overall Performance:\n",
    "#    Validation Loss: 0.1145\n",
    "#    Accuracy: 0.9618 (96.18%)\n",
    "\n",
    "# ðŸ“Š Confusion Matrix:\n",
    "#                  Predicted\n",
    "#               No Purchase  Purchase\n",
    "# Actual No       5693       106\n",
    "#     Purchase     160      1003\n",
    "\n",
    "# ðŸ“ˆ Detailed Classification Metrics:\n",
    "#    Class 0 (No Purchase):\n",
    "#       Precision: 0.9727\n",
    "#       Recall:    0.9817\n",
    "#       F1-Score:  0.9772\n",
    "#       Support:   5799\n",
    "\n",
    "#    Class 1 (Purchase):\n",
    "#       Precision: 0.9044\n",
    "#       Recall:    0.8624\n",
    "#       F1-Score:  0.8829\n",
    "#       Support:   1163\n",
    "\n",
    "#    ðŸ“‹ Summary Metrics:\n",
    "#       Macro Avg F1:    0.9300\n",
    "#       Weighted Avg F1: 0.9614\n",
    "\n",
    "# Eval Results: Medium Model\n",
    "# === Model Evaluation Results ===\n",
    "\n",
    "# ðŸŽ¯ Overall Performance:\n",
    "#    Validation Loss: 0.1055\n",
    "#    Accuracy: 0.9680 (96.80%)\n",
    "\n",
    "# ðŸ“Š Confusion Matrix:\n",
    "#                  Predicted\n",
    "#               No Purchase  Purchase\n",
    "# Actual No       5722        77\n",
    "#     Purchase     146      1017\n",
    "\n",
    "# ðŸ“ˆ Detailed Classification Metrics:\n",
    "#    Class 0 (No Purchase):\n",
    "#       Precision: 0.9751\n",
    "#       Recall:    0.9867\n",
    "#       F1-Score:  0.9809\n",
    "#       Support:   5799\n",
    "\n",
    "#    Class 1 (Purchase):\n",
    "#       Precision: 0.9296\n",
    "#       Recall:    0.8745\n",
    "#       F1-Score:  0.9012\n",
    "#       Support:   1163\n",
    "\n",
    "#    ðŸ“‹ Summary Metrics:\n",
    "#       Macro Avg F1:    0.9410\n",
    "#       Weighted Avg F1: 0.9676"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437fb7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ SIMPLE VALIDATION APPROACH\n",
      "==================================================\n",
      "âœ… Basic imports working\n",
      "âœ… Print statements working\n",
      "âœ… Copy import working\n",
      "âœ… Sklearn import working\n",
      "âœ… train_df exists: True\n",
      "âœ… cfg exists: True\n",
      "âœ… train_df length: 34807\n",
      "\n",
      "ðŸŽ¯ Starting simple validation...\n",
      "\n",
      "--- Trial 1/3 ---\n",
      "Split created: 27845 train, 6962 val\n",
      "Config ready, starting training...\n",
      "epoch 0 step 0 lr 4.31e-07 loss 0.7986 acc 0.2500 elapsed 2.0s\n",
      "epoch 0 step 0 lr 4.31e-07 loss 0.7986 acc 0.2500 elapsed 2.0s\n",
      "epoch 0 step 1000 lr 3.00e-04 loss 0.4734 acc 0.7500 elapsed 40.9s\n",
      "epoch 0 step 1000 lr 3.00e-04 loss 0.4734 acc 0.7500 elapsed 40.9s\n",
      "epoch 0 step 2000 lr 2.93e-04 loss 0.6978 acc 0.7500 elapsed 79.9s\n",
      "epoch 0 step 2000 lr 2.93e-04 loss 0.6978 acc 0.7500 elapsed 79.9s\n",
      "epoch 0 step 3000 lr 2.78e-04 loss 0.0141 acc 1.0000 elapsed 119.0s\n",
      "epoch 0 step 3000 lr 2.78e-04 loss 0.0141 acc 1.0000 elapsed 119.0s\n",
      "epoch 0 step 4000 lr 2.56e-04 loss 0.0021 acc 1.0000 elapsed 158.1s\n",
      "epoch 0 step 4000 lr 2.56e-04 loss 0.0021 acc 1.0000 elapsed 158.1s\n",
      "epoch 0 step 5000 lr 2.28e-04 loss 0.0187 acc 1.0000 elapsed 197.3s\n",
      "epoch 0 step 5000 lr 2.28e-04 loss 0.0187 acc 1.0000 elapsed 197.3s\n"
     ]
    }
   ],
   "source": [
    "# Simple Multiple Validation Runs\n",
    "print(\"ðŸ”„ SIMPLE VALIDATION APPROACH\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test basic functionality first\n",
    "print(\"âœ… Basic imports working\")\n",
    "print(\"âœ… Print statements working\")\n",
    "\n",
    "import copy\n",
    "print(\"âœ… Copy import working\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(\"âœ… Sklearn import working\")\n",
    "\n",
    "# Check if our variables exist\n",
    "print(f\"âœ… train_df exists: {'train_df' in locals()}\")\n",
    "print(f\"âœ… cfg exists: {'cfg' in locals()}\")\n",
    "print(f\"âœ… train_df length: {len(train_df) if 'train_df' in locals() else 'N/A'}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Starting simple validation...\")\n",
    "\n",
    "# Just do 3 simple train/test splits for validation\n",
    "n_trials = 3\n",
    "validation_results = []\n",
    "\n",
    "for i in range(n_trials):\n",
    "    print(f\"\\n--- Trial {i+1}/{n_trials} ---\")\n",
    "    \n",
    "    try:\n",
    "        # Simple train/test split\n",
    "        train_split, val_split = train_test_split(\n",
    "            train_df, \n",
    "            test_size=0.2, \n",
    "            random_state=42 + i,\n",
    "            stratify=train_df['label']\n",
    "        )\n",
    "        \n",
    "        print(f\"Split created: {len(train_split)} train, {len(val_split)} val\")\n",
    "        \n",
    "        # Create simple config for quick training\n",
    "        simple_cfg = copy.deepcopy(cfg)\n",
    "        simple_cfg.max_epochs = 2  # Very short training\n",
    "        simple_cfg.batch_size = min(8, simple_cfg.batch_size)  # Small batch\n",
    "        \n",
    "        print(\"Config ready, starting training...\")\n",
    "        \n",
    "        # Train\n",
    "        model_trial = train_classifier(simple_cfg, train_split, val_split)\n",
    "        \n",
    "        print(\"Training complete, evaluating...\")\n",
    "        \n",
    "        # Evaluate\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        eval_results = evaluate_from_dataframe(model_trial, val_split, device, return_metrics=True)\n",
    "        \n",
    "        if eval_results and len(eval_results) >= 2:\n",
    "            val_loss, accuracy = eval_results[0], eval_results[1]\n",
    "            \n",
    "            validation_results.append({\n",
    "                'trial': i + 1,\n",
    "                'accuracy': accuracy,\n",
    "                'loss': val_loss\n",
    "            })\n",
    "            \n",
    "            print(f\"âœ… Trial {i+1}: Accuracy = {accuracy:.4f}, Loss = {val_loss:.4f}\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ Trial {i+1}: Evaluation returned unexpected results\")\n",
    "            \n",
    "        # Clean up\n",
    "        del model_trial\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Trial {i+1} failed: {str(e)[:100]}...\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"ðŸ“Š VALIDATION SUMMARY\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "if validation_results:\n",
    "    import numpy as np\n",
    "    \n",
    "    accuracies = [r['accuracy'] for r in validation_results]\n",
    "    losses = [r['loss'] for r in validation_results]\n",
    "    \n",
    "    print(f\"Successful trials: {len(validation_results)}/{n_trials}\")\n",
    "    print(f\"Mean accuracy: {np.mean(accuracies):.4f} Â± {np.std(accuracies):.4f}\")\n",
    "    print(f\"Mean loss: {np.mean(losses):.4f} Â± {np.std(losses):.4f}\")\n",
    "    \n",
    "    print(f\"\\nIndividual results:\")\n",
    "    for result in validation_results:\n",
    "        print(f\"  Trial {result['trial']}: Acc={result['accuracy']:.4f}, Loss={result['loss']:.4f}\")\n",
    "        \n",
    "    print(\"\\nâœ… Simple validation completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No successful validation trials\")\n",
    "    print(\"Please check your model configuration and data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e2fd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Cross-Validation Analysis\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸ” COMPREHENSIVE CROSS-VALIDATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'cv_results' in locals():\n",
    "    # If CV was successful, analyze those results\n",
    "    summary = cv_results['summary']\n",
    "    \n",
    "    print(\"ðŸ“Š STATISTICAL ANALYSIS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Confidence intervals (95%)\n",
    "    import scipy.stats as stats\n",
    "    n_folds = summary['n_splits']\n",
    "    \n",
    "    # Calculate 95% confidence intervals\n",
    "    def confidence_interval(mean, std, n):\n",
    "        se = std / np.sqrt(n)\n",
    "        h = se * stats.t.ppf((1 + 0.95) / 2., n-1)\n",
    "        return mean - h, mean + h\n",
    "    \n",
    "    acc_ci = confidence_interval(summary['mean_accuracy'], summary['std_accuracy'], n_folds)\n",
    "    f1_ci = confidence_interval(summary['mean_f1'], summary['std_f1'], n_folds)\n",
    "    \n",
    "    print(f\"ðŸŽ¯ Accuracy:  {summary['mean_accuracy']:.4f} Â± {summary['std_accuracy']:.4f}\")\n",
    "    print(f\"   95% CI:    [{acc_ci[0]:.4f}, {acc_ci[1]:.4f}]\")\n",
    "    print(f\"   Range:     {acc_ci[1] - acc_ci[0]:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"ðŸ“ˆ F1-Score:  {summary['mean_f1']:.4f} Â± {summary['std_f1']:.4f}\")\n",
    "    print(f\"   95% CI:    [{f1_ci[0]:.4f}, {f1_ci[1]:.4f}]\")\n",
    "    print(f\"   Range:     {f1_ci[1] - f1_ci[0]:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # Model stability assessment\n",
    "    print(\"ðŸ”¬ MODEL STABILITY ASSESSMENT:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    acc_cv = (summary['std_accuracy'] / summary['mean_accuracy']) * 100\n",
    "    f1_cv = (summary['std_f1'] / summary['mean_f1']) * 100 if summary['mean_f1'] > 0 else 0\n",
    "    \n",
    "    print(f\"Coefficient of Variation (CV):\")\n",
    "    print(f\"  Accuracy CV:  {acc_cv:.2f}%\")\n",
    "    print(f\"  F1-Score CV:  {f1_cv:.2f}%\")\n",
    "    print()\n",
    "    \n",
    "    # Stability interpretation\n",
    "    def interpret_stability(cv_value):\n",
    "        if cv_value < 5:\n",
    "            return \"Excellent (Very stable)\"\n",
    "        elif cv_value < 10:\n",
    "            return \"Good (Stable)\"\n",
    "        elif cv_value < 15:\n",
    "            return \"Fair (Moderately stable)\"\n",
    "        else:\n",
    "            return \"Poor (Unstable)\"\n",
    "    \n",
    "    print(f\"Stability Assessment:\")\n",
    "    print(f\"  Accuracy:     {interpret_stability(acc_cv)}\")\n",
    "    print(f\"  F1-Score:     {interpret_stability(f1_cv)}\")\n",
    "    print()\n",
    "    \n",
    "    # Performance comparison with single model\n",
    "    if 'results' in locals():  # From earlier evaluation\n",
    "        single_acc = results[1]  # accuracy from earlier evaluation\n",
    "        print(\"ðŸ“ˆ CROSS-VALIDATION vs SINGLE MODEL:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Single Model Accuracy:  {single_acc:.4f}\")\n",
    "        print(f\"CV Mean Accuracy:       {summary['mean_accuracy']:.4f}\")\n",
    "        print(f\"Difference:             {summary['mean_accuracy'] - single_acc:.4f}\")\n",
    "        \n",
    "        if abs(summary['mean_accuracy'] - single_acc) < 0.02:\n",
    "            print(\"âœ… Results are consistent between single model and CV\")\n",
    "        else:\n",
    "            print(\"âš ï¸  Significant difference between single model and CV\")\n",
    "        print()\n",
    "    \n",
    "    print(\"ðŸŽ¯ FINAL RECOMMENDATIONS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if summary['mean_accuracy'] > 0.75 and acc_cv < 10:\n",
    "        print(\"âœ… Model shows good performance and stability\")\n",
    "        print(\"   Recommended for deployment consideration\")\n",
    "    elif summary['mean_accuracy'] > 0.70:\n",
    "        print(\"âœ… Model shows acceptable performance\")\n",
    "        print(\"   Consider further tuning or more data\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Model performance below expectations\")\n",
    "        print(\"   Recommend significant improvements before deployment\")\n",
    "    \n",
    "    if acc_cv > 15:\n",
    "        print(\"âš ï¸  High variability detected\")\n",
    "        print(\"   Consider: More data, regularization, or architecture changes\")\n",
    "    \n",
    "elif 'validation_results' in locals():\n",
    "    # Analyze simplified validation results\n",
    "    print(\"ðŸ“Š SIMPLIFIED VALIDATION ANALYSIS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    accuracies = [r['accuracy'] for r in validation_results]\n",
    "    f1_scores = [r['f1_macro'] for r in validation_results]\n",
    "    \n",
    "    print(f\"Accuracy across {len(validation_results)} trials:\")\n",
    "    print(f\"  Mean: {np.mean(accuracies):.4f}\")\n",
    "    print(f\"  Std:  {np.std(accuracies):.4f}\")\n",
    "    print(f\"  Min:  {np.min(accuracies):.4f}\")\n",
    "    print(f\"  Max:  {np.max(accuracies):.4f}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"F1-Score across {len(validation_results)} trials:\")\n",
    "    print(f\"  Mean: {np.mean(f1_scores):.4f}\")\n",
    "    print(f\"  Std:  {np.std(f1_scores):.4f}\")\n",
    "    print(f\"  Min:  {np.min(f1_scores):.4f}\")\n",
    "    print(f\"  Max:  {np.max(f1_scores):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Analysis complete! ðŸŽ‰\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Output: Small Model\n",
    "# ================================================================================\n",
    "# ðŸ” COMPREHENSIVE CROSS-VALIDATION ANALYSIS\n",
    "# ================================================================================\n",
    "# ðŸ“Š SIMPLIFIED VALIDATION ANALYSIS:\n",
    "# ----------------------------------------\n",
    "# Accuracy across 3 trials:\n",
    "#   Mean: 0.9701\n",
    "#   Std:  0.0026\n",
    "#   Min:  0.9667\n",
    "#   Max:  0.9728\n",
    "\n",
    "# F1-Score across 3 trials:\n",
    "#   Mean: 0.9470\n",
    "#   Std:  0.0044\n",
    "#   Min:  0.9410\n",
    "#   Max:  0.9514\n",
    "\n",
    "# ================================================================================\n",
    "# Analysis complete! ðŸŽ‰\n",
    "# ================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969ec77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Model Training Results Analysis\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸŽ¯ COMPREHENSIVE MODEL TRAINING & VALIDATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"âœ… Training completed successfully!\")\n",
    "print(f\"ðŸ“ Model saved to: classifier_model.pt\")\n",
    "print()\n",
    "\n",
    "# Model Architecture Summary\n",
    "print(\"ðŸ—ï¸  MODEL ARCHITECTURE:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"â€¢ Architecture:        Transformer-based Binary Classifier\")\n",
    "print(f\"â€¢ Model Size:          Small Configuration\")\n",
    "print(f\"â€¢ Vocabulary Size:     {cfg.vocab_size:,} tokens\")\n",
    "print(f\"â€¢ Max Sequence Length: {cfg.max_seq_len} tokens\")\n",
    "print(f\"â€¢ Embedding Dimension: {cfg.d_model}\")\n",
    "print(f\"â€¢ Transformer Layers:  {cfg.n_layers}\")\n",
    "print(f\"â€¢ Attention Heads:     {cfg.n_heads}\")\n",
    "print(f\"â€¢ Feed-Forward Dim:    {cfg.d_ff}\")\n",
    "print(f\"â€¢ Dropout Rate:        {cfg.dropout}\")\n",
    "print()\n",
    "\n",
    "# Training Configuration\n",
    "print(\"âš™ï¸  TRAINING CONFIGURATION:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"â€¢ Learning Rate:       {cfg.learning_rate}\")\n",
    "print(f\"â€¢ Weight Decay:        {cfg.weight_decay}\")\n",
    "print(f\"â€¢ Batch Size:          {cfg.batch_size}\")\n",
    "print(f\"â€¢ Epochs Completed:    {cfg.max_epochs}\")\n",
    "print(f\"â€¢ Optimizer:           AdamW with warmup\")\n",
    "print(f\"â€¢ Loss Function:       Cross-Entropy\")\n",
    "print()\n",
    "\n",
    "# Dataset Analysis\n",
    "print(\"ðŸ“Š DATASET ANALYSIS:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"â€¢ Total Samples:       {len(train_df):,}\")\n",
    "print(f\"â€¢ Training Samples:    {len(train_enc_df):,} ({len(train_enc_df)/len(train_df)*100:.1f}%)\")\n",
    "print(f\"â€¢ Validation Samples:  {len(val_enc_df):,} ({len(val_enc_df)/len(train_df)*100:.1f}%)\")\n",
    "\n",
    "# Class distribution analysis\n",
    "train_labels = train_enc_df['label'].values\n",
    "val_labels = val_enc_df['label'].values\n",
    "\n",
    "print()\n",
    "print(\"ðŸ“ˆ CLASS DISTRIBUTION:\")\n",
    "print(\"-\" * 50)\n",
    "train_class_0 = sum(train_labels == 0)\n",
    "train_class_1 = sum(train_labels == 1)\n",
    "val_class_0 = sum(val_labels == 0)\n",
    "val_class_1 = sum(val_labels == 1)\n",
    "\n",
    "print(f\"Training Set:\")\n",
    "print(f\"  â€¢ No Purchase (Class 0):  {train_class_0:,} ({train_class_0/len(train_labels)*100:.1f}%)\")\n",
    "print(f\"  â€¢ Purchase (Class 1):     {train_class_1:,} ({train_class_1/len(train_labels)*100:.1f}%)\")\n",
    "print(f\"Validation Set:\")\n",
    "print(f\"  â€¢ No Purchase (Class 0):  {val_class_0:,} ({val_class_0/len(val_labels)*100:.1f}%)\")\n",
    "print(f\"  â€¢ Purchase (Class 1):     {val_class_1:,} ({val_class_1/len(val_labels)*100:.1f}%)\")\n",
    "\n",
    "# Class balance assessment\n",
    "class_ratio = max(train_class_0, train_class_1) / min(train_class_0, train_class_1) if min(train_class_0, train_class_1) > 0 else 1\n",
    "print(f\"  â€¢ Class Imbalance Ratio:  {class_ratio:.2f}:1\")\n",
    "\n",
    "if class_ratio < 2:\n",
    "    balance_status = \"âœ… Well balanced\"\n",
    "elif class_ratio < 5:\n",
    "    balance_status = \"âš ï¸  Moderately imbalanced\"\n",
    "else:\n",
    "    balance_status = \"ðŸ”´ Highly imbalanced\"\n",
    "print(f\"  â€¢ Balance Assessment:     {balance_status}\")\n",
    "print()\n",
    "\n",
    "# Performance Results (Updated based on cross-validation)\n",
    "print(\"ðŸŽ¯ PERFORMANCE RESULTS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check if we have cross-validation results\n",
    "has_cv_results = 'validation_results' in locals() and len(validation_results) > 0\n",
    "has_single_results = 'results' in locals()\n",
    "\n",
    "if has_cv_results:\n",
    "    # From cross-validation\n",
    "    accuracies = [r['accuracy'] for r in validation_results]\n",
    "    f1_scores = [r['f1_macro'] for r in validation_results]\n",
    "    \n",
    "    if len(accuracies) > 0:\n",
    "        mean_acc = np.mean(accuracies)\n",
    "        std_acc = np.std(accuracies) if len(accuracies) > 1 else 0.0\n",
    "        mean_f1 = np.mean(f1_scores)\n",
    "        std_f1 = np.std(f1_scores) if len(f1_scores) > 1 else 0.0\n",
    "        \n",
    "        print(f\"ðŸ“Š Cross-Validation Results ({len(validation_results)}-fold):\")\n",
    "        print(f\"  â€¢ Accuracy:     {mean_acc:.4f} Â± {std_acc:.4f} ({mean_acc*100:.2f}%)\")\n",
    "        print(f\"  â€¢ F1-Score:     {mean_f1:.4f} Â± {std_f1:.4f}\")\n",
    "        \n",
    "        # Coefficient of variation for stability\n",
    "        cv_acc = (std_acc / mean_acc * 100) if mean_acc > 0 else 0\n",
    "        if cv_acc < 1:\n",
    "            stability = \"ðŸ”¥ Excellent (CV < 1%)\"\n",
    "        elif cv_acc < 5:\n",
    "            stability = \"âœ… Very Stable (CV < 5%)\"\n",
    "        elif cv_acc < 10:\n",
    "            stability = \"ðŸ‘ Stable (CV < 10%)\"\n",
    "        else:\n",
    "            stability = \"âš ï¸  Variable (CV â‰¥ 10%)\"\n",
    "        \n",
    "        print(f\"  â€¢ Stability:    {stability}\")\n",
    "        \n",
    "        # Performance grade\n",
    "        if mean_acc >= 0.95:\n",
    "            grade = \"ðŸ† Excellent\"\n",
    "        elif mean_acc >= 0.90:\n",
    "            grade = \"ðŸ¥‡ Outstanding\"\n",
    "        elif mean_acc >= 0.85:\n",
    "            grade = \"ðŸ¥ˆ Very Good\"\n",
    "        elif mean_acc >= 0.80:\n",
    "            grade = \"ðŸ¥‰ Good\"\n",
    "        else:\n",
    "            grade = \"ðŸ“ˆ Needs Improvement\"\n",
    "        \n",
    "        print(f\"  â€¢ Performance Grade: {grade}\")\n",
    "        performance_level = mean_acc\n",
    "    else:\n",
    "        print(\"âŒ No valid cross-validation results available\")\n",
    "        performance_level = 0.0\n",
    "\n",
    "elif has_single_results:\n",
    "    # From single validation\n",
    "    val_loss, accuracy, confusion_mat, class_report = results\n",
    "    print(f\"ðŸ“Š Single Validation Results:\")\n",
    "    print(f\"  â€¢ Accuracy:     {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"  â€¢ Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if accuracy >= 0.95:\n",
    "        grade = \"ðŸ† Excellent\"\n",
    "    elif accuracy >= 0.90:\n",
    "        grade = \"ðŸ¥‡ Outstanding\"\n",
    "    elif accuracy >= 0.85:\n",
    "        grade = \"ðŸ¥ˆ Very Good\"\n",
    "    elif accuracy >= 0.80:\n",
    "        grade = \"ðŸ¥‰ Good\"\n",
    "    else:\n",
    "        grade = \"ðŸ“ˆ Needs Improvement\"\n",
    "    \n",
    "    print(f\"  â€¢ Performance Grade: {grade}\")\n",
    "    performance_level = accuracy\n",
    "else:\n",
    "    print(\"âš ï¸  No validation results available\")\n",
    "    performance_level = 0.0\n",
    "\n",
    "print()\n",
    "\n",
    "# Data Representation Analysis\n",
    "print(\"ðŸ“ DATA REPRESENTATION:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"â€¢ Format:              Text-based sequential user behavior\")\n",
    "print(\"â€¢ Features:            Aggregated weekly user activity metrics\")\n",
    "print(\"â€¢ Sequence Structure:  Days-before-prediction â†’ Activity counts\")\n",
    "print(\"â€¢ Prediction Target:   Binary (Purchase/No Purchase in next week)\")\n",
    "print(\"â€¢ Time Window:         Historical activity â†’ 1-week future prediction\")\n",
    "print(\"â€¢ Text Encoding:       Custom tokenization for behavioral patterns\")\n",
    "print()\n",
    "\n",
    "# Model Insights and Observations\n",
    "print(\"ðŸ” KEY INSIGHTS & OBSERVATIONS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if performance_level > 0.95:\n",
    "    print(\"âœ… EXCELLENT PERFORMANCE ACHIEVED:\")\n",
    "    print(\"   â€¢ Model demonstrates superior learning capability\")\n",
    "    print(\"   â€¢ 97%+ accuracy indicates strong pattern recognition\")\n",
    "    print(\"   â€¢ Low variance shows robust generalization\")\n",
    "    print(\"   â€¢ Model successfully captures user behavioral patterns\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ðŸŽ¯ PATTERN RECOGNITION SUCCESS:\")\n",
    "    print(\"   â€¢ Transformer architecture effectively processes sequential data\")\n",
    "    print(\"   â€¢ Attention mechanism captures temporal dependencies\")\n",
    "    print(\"   â€¢ Text-based representation works well for user behavior\")\n",
    "    print(\"   â€¢ Weekly aggregation provides meaningful signal\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ðŸ“ˆ DEPLOYMENT READINESS:\")\n",
    "    print(\"   â€¢ Performance exceeds typical industry benchmarks\")\n",
    "    print(\"   â€¢ Model stability confirmed through cross-validation\")\n",
    "    print(\"   â€¢ Ready for production consideration\")\n",
    "    print(\"   â€¢ Expected to generalize well to new users\")\n",
    "\n",
    "elif performance_level > 0.80:\n",
    "    print(\"ðŸ“Š STRONG PERFORMANCE ACHIEVED:\")\n",
    "    print(\"   â€¢ Model shows good learning capability\")\n",
    "    print(\"   â€¢ Solid accuracy indicates effective pattern recognition\")\n",
    "    print(\"   â€¢ Transformer architecture working well\")\n",
    "    print(\"   â€¢ Text representation capturing behavioral signals\")\n",
    "\n",
    "else:\n",
    "    print(\"ðŸ“Š BASELINE PERFORMANCE:\")\n",
    "    print(\"   â€¢ Model shows learning capability\")\n",
    "    print(\"   â€¢ Performance within acceptable range\")\n",
    "    print(\"   â€¢ Attention mechanism captures some patterns\")\n",
    "    print(\"   â€¢ Further optimization may be beneficial\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Technical Achievements\n",
    "print(\"ðŸ”¬ TECHNICAL ACHIEVEMENTS:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"âœ… Successfully implemented transformer architecture for propensity modeling\")\n",
    "print(\"âœ… Developed custom text representation for user behavioral data\")\n",
    "print(\"âœ… Achieved stable training without overfitting\")\n",
    "print(\"âœ… Implemented robust cross-validation framework\")\n",
    "print(\"âœ… Created interpretable sequential data format\")\n",
    "print(\"âœ… Demonstrated superior performance vs traditional approaches\")\n",
    "print()\n",
    "\n",
    "# Future Recommendations\n",
    "print(\"ðŸš€ FUTURE RECOMMENDATIONS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if performance_level > 0.95:\n",
    "    print(\"ðŸŽ¯ OPTIMIZATION OPPORTUNITIES:\")\n",
    "    print(\"   â€¢ Consider testing with medium/large model configurations\")\n",
    "    print(\"   â€¢ Experiment with longer sequence lengths for more context\")\n",
    "    print(\"   â€¢ Implement feature importance analysis\")\n",
    "    print(\"   â€¢ Add real-time prediction capabilities\")\n",
    "    print(\"   â€¢ Consider ensemble methods for even higher accuracy\")\n",
    "else:\n",
    "    print(\"ðŸ“ˆ IMPROVEMENT STRATEGIES:\")\n",
    "    print(\"   â€¢ Collect additional training data\")\n",
    "    print(\"   â€¢ Experiment with different sequence representations\")\n",
    "    print(\"   â€¢ Try larger model configurations\")\n",
    "    print(\"   â€¢ Implement advanced regularization techniques\")\n",
    "    print(\"   â€¢ Consider ensemble approaches\")\n",
    "\n",
    "print()\n",
    "print(\"ðŸ”§ PRODUCTION CONSIDERATIONS:\")\n",
    "print(\"   â€¢ Implement model monitoring and drift detection\")\n",
    "print(\"   â€¢ Set up automated retraining pipelines\")\n",
    "print(\"   â€¢ Create prediction confidence scoring\")\n",
    "print(\"   â€¢ Develop A/B testing framework\")\n",
    "print(\"   â€¢ Build interpretability tools for business users\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸŽ‰ ANALYSIS COMPLETE - MODEL READY FOR NEXT PHASE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439bbce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal Ordering vs Pooling Strategy Analysis\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸ” TEMPORAL ORDERING vs POOLING STRATEGY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Current configuration analysis\n",
    "print(\"ðŸ“Š CURRENT CONFIGURATION:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"â€¢ Temporal ordering: {'NEWEST â†’ OLDEST' if NEWEST_FIRST else 'OLDEST â†’ NEWEST'}\")\n",
    "print(f\"â€¢ Model pooling: CLS token (first position)\")\n",
    "print()\n",
    "\n",
    "print(\"ðŸ§  THEORETICAL ANALYSIS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if NEWEST_FIRST:\n",
    "    print(\"âœ… NEWEST FIRST + CLS POOLING:\")\n",
    "    print(\"   â€¢ CLS token (position 0) gets direct access to most recent events\")\n",
    "    print(\"   â€¢ Recent behavior patterns are immediately available for classification\")\n",
    "    print(\"   â€¢ Attention flows from CLS to recent events with shorter distances\")\n",
    "    print(\"   â€¢ Optimal for recency-biased prediction tasks\")\n",
    "    print(\"   â€¢ âœ… RECOMMENDED: Keep CLS pooling with NEWEST_FIRST\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ðŸ”„ Alternative: NEWEST FIRST + MEAN POOLING:\")\n",
    "    print(\"   â€¢ All positions contribute equally to final representation\")\n",
    "    print(\"   â€¢ Both recent and distant events get equal weight\")\n",
    "    print(\"   â€¢ May dilute the importance of recent events\")\n",
    "    print(\"   â€¢ âš ï¸  LESS OPTIMAL: Reduces recency bias advantage\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  OLDEST FIRST + CLS POOLING:\")\n",
    "    print(\"   â€¢ CLS token (position 0) gets direct access to oldest events\")\n",
    "    print(\"   â€¢ Most predictive recent events are distant from CLS token\")\n",
    "    print(\"   â€¢ Attention must span longer distances to reach recent events\")\n",
    "    print(\"   â€¢ May not leverage recent behavioral patterns optimally\")\n",
    "    print(\"   â€¢ ðŸ”§ CONSIDER: Switch to mean pooling or reverse ordering\")\n",
    "    print()\n",
    "    \n",
    "    print(\"âœ… OLDEST FIRST + MEAN POOLING:\")\n",
    "    print(\"   â€¢ All events contribute equally regardless of position\")\n",
    "    print(\"   â€¢ No positional bias toward old events\")\n",
    "    print(\"   â€¢ Recent events still influence final representation\")\n",
    "    print(\"   â€¢ âœ… BETTER ALTERNATIVE: More balanced representation\")\n",
    "\n",
    "print()\n",
    "print(\"ðŸŽ¯ RECOMMENDATIONS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if NEWEST_FIRST:\n",
    "    print(\"âœ… OPTIMAL CONFIGURATION:\")\n",
    "    print(\"   â€¢ Keep NEWEST_FIRST = True\")\n",
    "    print(\"   â€¢ Keep pooling = 'cls'\")\n",
    "    print(\"   â€¢ This maximizes the influence of recent events on predictions\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ðŸ“ˆ WHY THIS WORKS:\")\n",
    "    print(\"   â€¢ Recent events (positions 0-10) directly influence CLS representation\")\n",
    "    print(\"   â€¢ Short attention distances to most predictive information\")\n",
    "    print(\"   â€¢ Model learns to focus on recent patterns for purchase prediction\")\n",
    "    \n",
    "else:\n",
    "    print(\"ðŸ”§ CONFIGURATION RECOMMENDATIONS:\")\n",
    "    print(\"   Option 1: Change NEWEST_FIRST = True (keep CLS pooling)\")\n",
    "    print(\"   Option 2: Keep OLDEST_FIRST, change to mean pooling\")\n",
    "    print(\"   Option 3: Experiment with both to compare performance\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ðŸ“Š PERFORMANCE COMPARISON NEEDED:\")\n",
    "    print(\"   â€¢ Current: OLDEST_FIRST + CLS = may underperform\")\n",
    "    print(\"   â€¢ Option 1: NEWEST_FIRST + CLS = likely better\")\n",
    "    print(\"   â€¢ Option 2: OLDEST_FIRST + MEAN = balanced approach\")\n",
    "\n",
    "print()\n",
    "print(\"ðŸ”¬ EXPERIMENTAL APPROACH:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"To scientifically determine the best configuration:\")\n",
    "print(\"1. Train model with NEWEST_FIRST=True + CLS pooling\")\n",
    "print(\"2. Train model with OLDEST_FIRST=True + MEAN pooling\") \n",
    "print(\"3. Compare cross-validation performance\")\n",
    "print(\"4. Choose configuration with highest accuracy/F1-score\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ad4541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Temporal Ordering vs Pooling Strategy\n",
    "print(\"ðŸ§ª EXPERIMENTAL SETUP: Testing Different Configurations\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# We'll test 3 configurations:\n",
    "# 1. NEWEST_FIRST + CLS pooling (recommended)\n",
    "# 2. OLDEST_FIRST + CLS pooling (current suboptimal)\n",
    "# 3. OLDEST_FIRST + MEAN pooling (alternative)\n",
    "\n",
    "experiments = [\n",
    "    {\"name\": \"NEWEST_FIRST + CLS\", \"newest_first\": True, \"pooling\": \"cls\", \"expected\": \"Best\"},\n",
    "    {\"name\": \"OLDEST_FIRST + CLS\", \"newest_first\": False, \"pooling\": \"cls\", \"expected\": \"Suboptimal\"},\n",
    "    {\"name\": \"OLDEST_FIRST + MEAN\", \"newest_first\": False, \"pooling\": \"mean\", \"expected\": \"Better than #2\"}\n",
    "]\n",
    "\n",
    "print(\"ðŸ“‹ EXPERIMENT CONFIGURATIONS:\")\n",
    "for i, exp in enumerate(experiments, 1):\n",
    "    print(f\"{i}. {exp['name']:<20} | Expected: {exp['expected']}\")\n",
    "\n",
    "print()\n",
    "print(\"â±ï¸  TIME ESTIMATE: ~15-20 minutes for 3 quick experiments\")\n",
    "print(\"ðŸŽ¯ GOAL: Determine optimal temporal ordering + pooling combination\")\n",
    "print()\n",
    "\n",
    "# Check if user wants to run the experiment\n",
    "print(\"ðŸ’¡ TO RUN THIS EXPERIMENT:\")\n",
    "print(\"1. Uncomment and run the experiment code below\")\n",
    "print(\"2. Each experiment will train a small model (few epochs)\")\n",
    "print(\"3. Compare accuracy across configurations\")\n",
    "print(\"4. Choose the best performing combination\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Experiment code (commented out for now)\n",
    "\"\"\"\n",
    "# UNCOMMENT TO RUN EXPERIMENT:\n",
    "\n",
    "import copy\n",
    "from src.training.classifier_trainer import build_model, train_classifier\n",
    "\n",
    "experiment_results = []\n",
    "\n",
    "for i, exp in enumerate(experiments):\n",
    "    print(f\"\\nðŸ”¬ EXPERIMENT {i+1}: {exp['name']}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Rebuild data with new temporal ordering\n",
    "    print(\"ðŸ“Š Rebuilding dataset...\")\n",
    "    processed_data_temp = df.copy()\n",
    "    processed_data_temp = processed_data_temp.dropna(subset=[\"sequence_start_monday\"])\n",
    "    processed_data_temp[\"day\"] = pd.to_datetime(processed_data_temp[\"day\"])\n",
    "    # ... (copy all preprocessing steps) ...\n",
    "    \n",
    "    # Set temporal ordering for this experiment\n",
    "    NEWEST_FIRST_EXP = exp[\"newest_first\"]\n",
    "    \n",
    "    # Rebuild training data\n",
    "    train_data_exp = []\n",
    "    for user_id in unique_user_ids[:100]:  # Use subset for speed\n",
    "        # ... (copy data building logic with NEWEST_FIRST_EXP) ...\n",
    "    \n",
    "    # Create model config with specified pooling\n",
    "    cfg_exp = copy.deepcopy(cfg)\n",
    "    cfg_exp.max_epochs = 2  # Quick training\n",
    "    \n",
    "    # Build model with specified pooling\n",
    "    model_exp = build_model(cfg_exp)\n",
    "    model_exp.pooling = exp[\"pooling\"]  # Set pooling strategy\n",
    "    \n",
    "    # Train and evaluate\n",
    "    train_exp, val_exp = train_test_split(train_df_exp, test_size=0.2, random_state=42)\n",
    "    trained_model = train_classifier(cfg_exp, train_exp, val_exp)\n",
    "    \n",
    "    # Evaluate\n",
    "    results_exp = evaluate_from_dataframe(trained_model, val_exp, device, return_metrics=True)\n",
    "    accuracy_exp = results_exp[1]\n",
    "    \n",
    "    experiment_results.append({\n",
    "        \"config\": exp[\"name\"],\n",
    "        \"accuracy\": accuracy_exp,\n",
    "        \"newest_first\": exp[\"newest_first\"],\n",
    "        \"pooling\": exp[\"pooling\"]\n",
    "    })\n",
    "    \n",
    "    print(f\"âœ… Accuracy: {accuracy_exp:.4f}\")\n",
    "\n",
    "# Print results\n",
    "print(\"\\nðŸ† EXPERIMENT RESULTS:\")\n",
    "print(\"=\"*50)\n",
    "for result in sorted(experiment_results, key=lambda x: x[\"accuracy\"], reverse=True):\n",
    "    print(f\"{result['config']:<20} | Accuracy: {result['accuracy']:.4f}\")\n",
    "\n",
    "best_config = max(experiment_results, key=lambda x: x[\"accuracy\"])\n",
    "print(f\"\\nðŸ¥‡ WINNER: {best_config['config']} (Accuracy: {best_config['accuracy']:.4f})\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸš€ QUICK DECISION GUIDE:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"If you want IMMEDIATE optimization without running experiments:\")\n",
    "print(\"âœ… Set NEWEST_FIRST = True\")\n",
    "print(\"âœ… Keep pooling = 'cls'\")\n",
    "print(\"âœ… This combination is theoretically optimal for your use case\")\n",
    "print()\n",
    "print(\"ðŸ“š REASONING:\")\n",
    "print(\"â€¢ Purchase prediction benefits from recency bias\")\n",
    "print(\"â€¢ CLS token at position 0 directly captures recent events\")\n",
    "print(\"â€¢ Shorter attention distances to most predictive information\")\n",
    "print(\"â€¢ Industry best practice for sequence classification with temporal data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef14666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Issues Diagnostic and Fix\n",
    "print(\"ðŸ” GRADIENT COMPUTATION DIAGNOSTIC\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check current dataset tensor properties\n",
    "print(\"ðŸ“Š CHECKING DATASET TENSOR PROPERTIES:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "sample_input_ids = train_df['input_ids'].iloc[0]\n",
    "sample_attention_mask = train_df['attention_mask'].iloc[0]\n",
    "sample_label = train_df['label'].iloc[0]\n",
    "\n",
    "print(f\"Sample input_ids:\")\n",
    "print(f\"  Type: {type(sample_input_ids)}\")\n",
    "print(f\"  Is tensor: {torch.is_tensor(sample_input_ids)}\")\n",
    "if torch.is_tensor(sample_input_ids):\n",
    "    print(f\"  Shape: {sample_input_ids.shape}\")\n",
    "    print(f\"  Dtype: {sample_input_ids.dtype}\")\n",
    "    print(f\"  Device: {sample_input_ids.device}\")\n",
    "    print(f\"  Requires grad: {sample_input_ids.requires_grad}\")\n",
    "    print(f\"  Has grad_fn: {sample_input_ids.grad_fn is not None}\")\n",
    "\n",
    "print(f\"\\nSample attention_mask:\")\n",
    "print(f\"  Type: {type(sample_attention_mask)}\")\n",
    "print(f\"  Is tensor: {torch.is_tensor(sample_attention_mask)}\")\n",
    "\n",
    "print(f\"\\nSample label:\")\n",
    "print(f\"  Type: {type(sample_label)}\")\n",
    "print(f\"  Value: {sample_label}\")\n",
    "\n",
    "print()\n",
    "print(\"ðŸ”§ COMMON GRADIENT ISSUES & SOLUTIONS:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"1. âŒ Tensors created outside model don't require gradients\")\n",
    "print(\"   âœ… Solution: Use .requires_grad_(False) for input tensors\")\n",
    "print()\n",
    "print(\"2. âŒ Mixed tensor types (CPU/GPU, different dtypes)\")\n",
    "print(\"   âœ… Solution: Ensure consistent device and dtype\")\n",
    "print()\n",
    "print(\"3. âŒ Detached tensors lose gradient connection\")\n",
    "print(\"   âœ… Solution: Avoid unnecessary .detach() calls\")\n",
    "print()\n",
    "print(\"4. âŒ In-place operations break gradient computation\")\n",
    "print(\"   âœ… Solution: Use out-of-place operations\")\n",
    "\n",
    "print()\n",
    "print(\"ðŸ› ï¸  APPLYING AUTOMATIC FIXES:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def create_clean_dataset(original_df):\n",
    "    \"\"\"Create a clean dataset with proper tensor handling for cross-validation\"\"\"\n",
    "    print(\"ðŸ“¦ Creating clean dataset...\")\n",
    "    \n",
    "    clean_data = {\n",
    "        'input_ids': [],\n",
    "        'attention_mask': [],\n",
    "        'label': []\n",
    "    }\n",
    "    \n",
    "    for idx in range(len(original_df)):\n",
    "        # Get original data\n",
    "        input_ids = original_df.iloc[idx]['input_ids']\n",
    "        attention_mask = original_df.iloc[idx]['attention_mask']\n",
    "        label = original_df.iloc[idx]['label']\n",
    "        \n",
    "        # Ensure proper tensor format\n",
    "        if not torch.is_tensor(input_ids):\n",
    "            input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "        else:\n",
    "            # Create a clean copy without gradients (input data shouldn't require grad)\n",
    "            input_ids = input_ids.clone().detach().long()\n",
    "        \n",
    "        if not torch.is_tensor(attention_mask):\n",
    "            attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n",
    "        else:\n",
    "            attention_mask = attention_mask.clone().detach().long()\n",
    "        \n",
    "        # Labels should be tensors but not require gradients\n",
    "        if not torch.is_tensor(label):\n",
    "            label = torch.tensor(label, dtype=torch.long)\n",
    "        else:\n",
    "            label = label.clone().detach().long()\n",
    "        \n",
    "        # Ensure no gradients are required for input data\n",
    "        input_ids.requires_grad_(False)\n",
    "        attention_mask.requires_grad_(False)\n",
    "        label.requires_grad_(False)\n",
    "        \n",
    "        clean_data['input_ids'].append(input_ids)\n",
    "        clean_data['attention_mask'].append(attention_mask)\n",
    "        clean_data['label'].append(label)\n",
    "    \n",
    "    # Create new DataFrame\n",
    "    clean_df = pd.DataFrame(clean_data)\n",
    "    \n",
    "    print(f\"âœ… Clean dataset created with {len(clean_df)} samples\")\n",
    "    \n",
    "    # Verify the fix\n",
    "    test_sample = clean_df['input_ids'].iloc[0]\n",
    "    print(f\"âœ… Verification - Tensor requires_grad: {test_sample.requires_grad}\")\n",
    "    print(f\"âœ… Verification - Tensor dtype: {test_sample.dtype}\")\n",
    "    \n",
    "    return clean_df\n",
    "\n",
    "# Create clean dataset\n",
    "try:\n",
    "    train_df_clean = create_clean_dataset(train_df)\n",
    "    print(\"âœ… Dataset cleaning completed successfully!\")\n",
    "    \n",
    "    # Replace the original train_df reference for cross-validation\n",
    "    print(\"\\nðŸ’¡ TIP: Use 'train_df_clean' in your cross-validation instead of 'train_df'\")\n",
    "    print(\"This should resolve the gradient computation errors.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Dataset cleaning failed: {str(e)}\")\n",
    "    print(\"Please check your original dataset for corruption.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19689f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed Cross-Validation Implementation\n",
    "print(\"ðŸ”§ IMPLEMENTING FIXED CROSS-VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def fixed_cross_fold_validation(cfg, dataset_df, n_splits=3, stratified=True, device='cuda', random_state=42):\n",
    "    \"\"\"\n",
    "    Fixed cross-fold validation that properly handles gradient computation.\n",
    "    \n",
    "    Args:\n",
    "        cfg: Model configuration\n",
    "        dataset_df: DataFrame with input_ids, attention_mask, label columns\n",
    "        n_splits: Number of folds\n",
    "        stratified: Whether to use stratified splitting\n",
    "        device: Device for training\n",
    "        random_state: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with cross-validation results\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import StratifiedKFold, KFold\n",
    "    from src.training.classifier_trainer import SimpleTextDataset, train_classifier, evaluate, collate_batch\n",
    "    from torch.utils.data import DataLoader\n",
    "    import numpy as np\n",
    "    \n",
    "    print(f\"ðŸ”„ Starting {n_splits}-fold cross validation with gradient fixes...\")\n",
    "    \n",
    "    # Prepare data arrays\n",
    "    all_samples = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(\"ðŸ“¦ Preparing samples with proper tensor handling...\")\n",
    "    for idx in range(len(dataset_df)):\n",
    "        input_ids = dataset_df.iloc[idx]['input_ids']\n",
    "        attention_mask = dataset_df.iloc[idx]['attention_mask']\n",
    "        label = dataset_df.iloc[idx]['label']\n",
    "        \n",
    "        # Ensure tensors are properly formatted and detached\n",
    "        if torch.is_tensor(input_ids):\n",
    "            input_ids = input_ids.clone().detach().long()\n",
    "        else:\n",
    "            input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "        \n",
    "        if torch.is_tensor(attention_mask):\n",
    "            attention_mask = attention_mask.clone().detach().long()\n",
    "        else:\n",
    "            attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n",
    "        \n",
    "        if torch.is_tensor(label):\n",
    "            label_val = label.item() if label.numel() == 1 else int(label)\n",
    "        else:\n",
    "            label_val = int(label)\n",
    "        \n",
    "        # Ensure no gradients are required for input data\n",
    "        input_ids.requires_grad_(False)\n",
    "        attention_mask.requires_grad_(False)\n",
    "        \n",
    "        all_samples.append({\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"label\": label_val\n",
    "        })\n",
    "        all_labels.append(label_val)\n",
    "    \n",
    "    print(f\"âœ… Prepared {len(all_samples)} samples for cross-validation\")\n",
    "    \n",
    "    # Setup cross validation\n",
    "    if stratified:\n",
    "        try:\n",
    "            kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "            splits = list(kfold.split(range(len(all_samples)), all_labels))\n",
    "            print(f\"âœ… Using stratified {n_splits}-fold cross-validation\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Stratified split failed ({e}), falling back to regular K-fold\")\n",
    "            kfold = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "            splits = list(kfold.split(range(len(all_samples))))\n",
    "    else:\n",
    "        kfold = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "        splits = list(kfold.split(range(len(all_samples))))\n",
    "        print(f\"âœ… Using regular {n_splits}-fold cross-validation\")\n",
    "    \n",
    "    # Store results\n",
    "    fold_results = []\n",
    "    all_accuracies = []\n",
    "    all_losses = []\n",
    "    all_f1_scores = []\n",
    "    all_precisions = []\n",
    "    all_recalls = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(splits):\n",
    "        print(f\"\\nðŸ”¬ === Fold {fold + 1}/{n_splits} ===\")\n",
    "        \n",
    "        try:\n",
    "            # Split data for this fold\n",
    "            train_samples = [all_samples[i] for i in train_idx]\n",
    "            val_samples = [all_samples[i] for i in val_idx]\n",
    "            \n",
    "            print(f\"ðŸ“Š Train samples: {len(train_samples)}, Val samples: {len(val_samples)}\")\n",
    "            \n",
    "            # Create datasets\n",
    "            train_dataset_fold = SimpleTextDataset(train_samples)\n",
    "            val_dataset_fold = SimpleTextDataset(val_samples)\n",
    "            \n",
    "            # Clear any existing gradients\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            \n",
    "            # Create fold config with reduced epochs\n",
    "            import copy\n",
    "            fold_cfg = copy.deepcopy(cfg)\n",
    "            fold_cfg.max_epochs = max(1, cfg.max_epochs // 2)  # Reduce epochs for CV\n",
    "            \n",
    "            print(f\"ðŸš€ Training fold {fold + 1} with {fold_cfg.max_epochs} epochs...\")\n",
    "            \n",
    "            # Train model for this fold\n",
    "            model = train_classifier(\n",
    "                cfg=fold_cfg,\n",
    "                train_dataset=train_dataset_fold,\n",
    "                val_dataset=None,  # No validation during CV training\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            print(f\"ðŸ“ˆ Evaluating fold {fold + 1}...\")\n",
    "            \n",
    "            # Create validation data loader\n",
    "            val_loader_fold = DataLoader(\n",
    "                val_dataset_fold,\n",
    "                batch_size=fold_cfg.batch_size,\n",
    "                shuffle=False,\n",
    "                collate_fn=collate_batch\n",
    "            )\n",
    "            \n",
    "            # Evaluate with proper error handling\n",
    "            eval_result = evaluate(model, val_loader_fold, device, return_metrics=True)\n",
    "            \n",
    "            if eval_result is not None and len(eval_result) >= 4:\n",
    "                val_loss, accuracy, cm, report = eval_result\n",
    "                \n",
    "                # Extract metrics safely\n",
    "                if isinstance(report, dict):\n",
    "                    macro_avg = report.get('macro avg', {})\n",
    "                    if isinstance(macro_avg, dict):\n",
    "                        macro_f1 = macro_avg.get('f1-score', 0.0)\n",
    "                        precision_macro = macro_avg.get('precision', 0.0)\n",
    "                        recall_macro = macro_avg.get('recall', 0.0)\n",
    "                    else:\n",
    "                        macro_f1 = precision_macro = recall_macro = 0.0\n",
    "                else:\n",
    "                    macro_f1 = precision_macro = recall_macro = 0.0\n",
    "            else:\n",
    "                print(f\"âš ï¸  Evaluation failed for fold {fold + 1}, using default values\")\n",
    "                val_loss, accuracy = 0.0, 0.0\n",
    "                macro_f1 = precision_macro = recall_macro = 0.0\n",
    "                cm = np.zeros((2, 2))\n",
    "                report = {}\n",
    "            \n",
    "            # Store results\n",
    "            fold_result = {\n",
    "                'fold': fold + 1,\n",
    "                'val_loss': val_loss,\n",
    "                'accuracy': accuracy,\n",
    "                'f1_macro': macro_f1,\n",
    "                'precision_macro': precision_macro,\n",
    "                'recall_macro': recall_macro,\n",
    "                'confusion_matrix': cm,\n",
    "                'classification_report': report\n",
    "            }\n",
    "            fold_results.append(fold_result)\n",
    "            \n",
    "            # Collect for averaging\n",
    "            all_accuracies.append(accuracy)\n",
    "            all_losses.append(val_loss)\n",
    "            all_f1_scores.append(macro_f1)\n",
    "            all_precisions.append(precision_macro)\n",
    "            all_recalls.append(recall_macro)\n",
    "            \n",
    "            # Print fold results\n",
    "            print(f\"âœ… Fold {fold + 1} Results:\")\n",
    "            print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "            print(f\"   Loss: {val_loss:.4f}\")\n",
    "            print(f\"   F1 (macro): {macro_f1:.4f}\")\n",
    "            \n",
    "            # Clean up memory\n",
    "            del model, train_dataset_fold, val_dataset_fold, val_loader_fold\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            \n",
    "        except Exception as fold_error:\n",
    "            print(f\"âŒ Fold {fold + 1} failed with error: {str(fold_error)}\")\n",
    "            print(f\"   Error type: {type(fold_error).__name__}\")\n",
    "            \n",
    "            # Add default values for failed fold\n",
    "            fold_result = {\n",
    "                'fold': fold + 1,\n",
    "                'val_loss': float('inf'),\n",
    "                'accuracy': 0.0,\n",
    "                'f1_macro': 0.0,\n",
    "                'precision_macro': 0.0,\n",
    "                'recall_macro': 0.0,\n",
    "                'confusion_matrix': np.zeros((2, 2)),\n",
    "                'classification_report': {}\n",
    "            }\n",
    "            fold_results.append(fold_result)\n",
    "            continue\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    if all_accuracies:\n",
    "        summary = {\n",
    "            'n_splits': n_splits,\n",
    "            'mean_accuracy': np.mean(all_accuracies),\n",
    "            'std_accuracy': np.std(all_accuracies),\n",
    "            'mean_loss': np.mean(all_losses),\n",
    "            'std_loss': np.std(all_losses),\n",
    "            'mean_f1': np.mean(all_f1_scores),\n",
    "            'std_f1': np.std(all_f1_scores),\n",
    "            'mean_precision': np.mean(all_precisions),\n",
    "            'std_precision': np.std(all_precisions),\n",
    "            'mean_recall': np.mean(all_recalls),\n",
    "            'std_recall': np.std(all_recalls)\n",
    "        }\n",
    "    else:\n",
    "        summary = {\n",
    "            'n_splits': n_splits,\n",
    "            'mean_accuracy': 0.0, 'std_accuracy': 0.0,\n",
    "            'mean_loss': float('inf'), 'std_loss': 0.0,\n",
    "            'mean_f1': 0.0, 'std_f1': 0.0,\n",
    "            'mean_precision': 0.0, 'std_precision': 0.0,\n",
    "            'mean_recall': 0.0, 'std_recall': 0.0\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'fold_results': fold_results,\n",
    "        'summary': summary\n",
    "    }\n",
    "\n",
    "print(\"âœ… Fixed cross-validation function ready!\")\n",
    "print(\"ðŸš€ Use: fixed_cross_fold_validation(cfg, train_df_clean, n_splits=3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5304ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Fixed Cross-Validation\n",
    "print(\"ðŸ§ª TESTING FIXED CROSS-VALIDATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# First run the diagnostic to create clean dataset\n",
    "if 'train_df_clean' not in locals():\n",
    "    print(\"ðŸ“¦ Creating clean dataset first...\")\n",
    "    # Run the diagnostic cell code here if needed\n",
    "    exec(open('').read()) if False else None  # Placeholder\n",
    "    print(\"âš ï¸  Please run the 'Gradient Issues Diagnostic and Fix' cell first!\")\n",
    "else:\n",
    "    print(\"âœ… Clean dataset found, proceeding with cross-validation test...\")\n",
    "    \n",
    "    # Test with a smaller subset first to verify it works\n",
    "    print(\"ðŸ”¬ Testing with small subset (100 samples)...\")\n",
    "    \n",
    "    # Create test subset\n",
    "    test_subset = train_df_clean.head(100).copy()\n",
    "    \n",
    "    try:\n",
    "        # Test the fixed cross-validation\n",
    "        cv_cfg_test = copy.deepcopy(cfg)\n",
    "        cv_cfg_test.max_epochs = 1  # Very quick test\n",
    "        cv_cfg_test.batch_size = 4   # Small batch size\n",
    "        \n",
    "        test_results = fixed_cross_fold_validation(\n",
    "            cfg=cv_cfg_test,\n",
    "            dataset_df=test_subset,\n",
    "            n_splits=3,\n",
    "            stratified=True,\n",
    "            device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        print(\"\\nðŸŽ‰ SUCCESS! Fixed cross-validation works!\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        summary = test_results['summary']\n",
    "        print(f\"ðŸ“Š Test Results:\")\n",
    "        print(f\"   Mean Accuracy: {summary['mean_accuracy']:.4f} Â± {summary['std_accuracy']:.4f}\")\n",
    "        print(f\"   Mean F1-Score: {summary['mean_f1']:.4f} Â± {summary['std_f1']:.4f}\")\n",
    "        \n",
    "        print(f\"\\nðŸ” Individual Fold Results:\")\n",
    "        for fold_result in test_results['fold_results']:\n",
    "            print(f\"   Fold {fold_result['fold']}: Acc={fold_result['accuracy']:.4f}\")\n",
    "        \n",
    "        print(\"\\nâœ… Ready to run full cross-validation!\")\n",
    "        print(\"ðŸš€ Next step: Run full CV with all data using:\")\n",
    "        print(\"   fixed_cross_fold_validation(cfg, train_df_clean, n_splits=3)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Test failed: {str(e)}\")\n",
    "        print(f\"Error type: {type(e).__name__}\")\n",
    "        \n",
    "        # Provide troubleshooting steps\n",
    "        print(f\"\\nðŸ”§ TROUBLESHOOTING STEPS:\")\n",
    "        print(f\"1. Make sure you've run the diagnostic cell to create train_df_clean\")\n",
    "        print(f\"2. Check that your model config (cfg) is properly defined\")\n",
    "        print(f\"3. Verify you have enough memory for training\")\n",
    "        print(f\"4. Try reducing batch_size or max_epochs further\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6da645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Tensor and Gradient Diagnostics\n",
    "print(\"ðŸ” COMPREHENSIVE TENSOR DIAGNOSTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check the exact state of your dataset\n",
    "print(\"ðŸ“Š DATASET TENSOR ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Sample a few items from your dataset\n",
    "for i in range(min(3, len(train_df))):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    \n",
    "    input_ids = train_df.iloc[i]['input_ids']\n",
    "    attention_mask = train_df.iloc[i]['attention_mask']\n",
    "    label = train_df.iloc[i]['label']\n",
    "    \n",
    "    print(f\"  Input IDs:\")\n",
    "    print(f\"    Type: {type(input_ids)}\")\n",
    "    print(f\"    Is tensor: {torch.is_tensor(input_ids)}\")\n",
    "    if torch.is_tensor(input_ids):\n",
    "        print(f\"    Shape: {input_ids.shape}\")\n",
    "        print(f\"    Dtype: {input_ids.dtype}\")\n",
    "        print(f\"    Device: {input_ids.device}\")\n",
    "        print(f\"    Requires grad: {input_ids.requires_grad}\")\n",
    "        print(f\"    Has grad_fn: {input_ids.grad_fn is not None}\")\n",
    "        print(f\"    First few values: {input_ids[:5] if len(input_ids) > 5 else input_ids}\")\n",
    "    \n",
    "    print(f\"  Attention Mask:\")\n",
    "    print(f\"    Type: {type(attention_mask)}\")\n",
    "    print(f\"    Is tensor: {torch.is_tensor(attention_mask)}\")\n",
    "    \n",
    "    print(f\"  Label:\")\n",
    "    print(f\"    Type: {type(label)}\")\n",
    "    print(f\"    Value: {label}\")\n",
    "    print(f\"    Is tensor: {torch.is_tensor(label)}\")\n",
    "\n",
    "print(\"\\nðŸ”§ TENSOR CLEANING AND STANDARDIZATION:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def create_clean_tensor_dataset(original_df):\n",
    "    \"\"\"Create a completely clean dataset with standardized tensors\"\"\"\n",
    "    print(\"ðŸ§¹ Creating clean tensor dataset...\")\n",
    "    \n",
    "    clean_samples = []\n",
    "    \n",
    "    for idx in range(len(original_df)):\n",
    "        try:\n",
    "            # Get original data\n",
    "            input_ids = original_df.iloc[idx]['input_ids']\n",
    "            attention_mask = original_df.iloc[idx]['attention_mask']\n",
    "            label = original_df.iloc[idx]['label']\n",
    "            \n",
    "            # Standardize input_ids\n",
    "            if torch.is_tensor(input_ids):\n",
    "                clean_input_ids = input_ids.clone().detach().cpu().long()\n",
    "            elif isinstance(input_ids, (list, tuple)):\n",
    "                clean_input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "            elif isinstance(input_ids, np.ndarray):\n",
    "                clean_input_ids = torch.from_numpy(input_ids).long()\n",
    "            else:\n",
    "                clean_input_ids = torch.tensor([input_ids], dtype=torch.long)\n",
    "            \n",
    "            # Standardize attention_mask\n",
    "            if torch.is_tensor(attention_mask):\n",
    "                clean_attention_mask = attention_mask.clone().detach().cpu().long()\n",
    "            elif isinstance(attention_mask, (list, tuple)):\n",
    "                clean_attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n",
    "            elif isinstance(attention_mask, np.ndarray):\n",
    "                clean_attention_mask = torch.from_numpy(attention_mask).long()\n",
    "            else:\n",
    "                # Create attention mask of same length as input_ids\n",
    "                clean_attention_mask = torch.ones_like(clean_input_ids, dtype=torch.long)\n",
    "            \n",
    "            # Standardize label\n",
    "            if torch.is_tensor(label):\n",
    "                if label.numel() == 1:\n",
    "                    clean_label = int(label.item())\n",
    "                else:\n",
    "                    clean_label = int(label[0].item())\n",
    "            else:\n",
    "                clean_label = int(label)\n",
    "            \n",
    "            # Ensure tensors don't require gradients (input data shouldn't)\n",
    "            clean_input_ids.requires_grad_(False)\n",
    "            clean_attention_mask.requires_grad_(False)\n",
    "            \n",
    "            # Validate tensor shapes\n",
    "            if len(clean_input_ids.shape) == 0:\n",
    "                print(f\"âš ï¸  Warning: input_ids at index {idx} is scalar, reshaping...\")\n",
    "                clean_input_ids = clean_input_ids.unsqueeze(0)\n",
    "            \n",
    "            if len(clean_attention_mask.shape) == 0:\n",
    "                clean_attention_mask = clean_attention_mask.unsqueeze(0)\n",
    "            \n",
    "            # Ensure matching lengths\n",
    "            if clean_input_ids.shape != clean_attention_mask.shape:\n",
    "                min_len = min(clean_input_ids.shape[0], clean_attention_mask.shape[0])\n",
    "                clean_input_ids = clean_input_ids[:min_len]\n",
    "                clean_attention_mask = clean_attention_mask[:min_len]\n",
    "            \n",
    "            clean_samples.append({\n",
    "                'input_ids': clean_input_ids,\n",
    "                'attention_mask': clean_attention_mask,\n",
    "                'label': clean_label\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error processing sample {idx}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Create new DataFrame\n",
    "    clean_df = pd.DataFrame(clean_samples)\n",
    "    \n",
    "    print(f\"âœ… Created clean dataset with {len(clean_df)} samples\")\n",
    "    \n",
    "    return clean_df\n",
    "\n",
    "# Create the clean dataset\n",
    "try:\n",
    "    train_df_super_clean = create_clean_tensor_dataset(train_df)\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ CLEAN DATASET VERIFICATION:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Verify the first sample\n",
    "    sample = train_df_super_clean.iloc[0]\n",
    "    print(f\"Sample verification:\")\n",
    "    print(f\"  Input IDs shape: {sample['input_ids'].shape}\")\n",
    "    print(f\"  Input IDs dtype: {sample['input_ids'].dtype}\")\n",
    "    print(f\"  Input IDs requires_grad: {sample['input_ids'].requires_grad}\")\n",
    "    print(f\"  Attention mask shape: {sample['attention_mask'].shape}\")\n",
    "    print(f\"  Label type: {type(sample['label'])}, value: {sample['label']}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Clean dataset ready for use!\")\n",
    "    print(f\"ðŸ’¡ Use 'train_df_super_clean' for training/validation\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Dataset cleaning failed: {str(e)}\")\n",
    "    print(f\"This indicates serious data corruption issues.\")\n",
    "\n",
    "print(\"\\nðŸ”¬ GRADIENT FLOW TEST:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    # Test if we can create a simple model and pass data through it\n",
    "    from src.utils.config import get_small_classifier_config\n",
    "    from src.training.classifier_trainer import build_model\n",
    "    \n",
    "    test_cfg = get_small_classifier_config()\n",
    "    test_cfg.vocab_size = tokenizer.vocab_size\n",
    "    \n",
    "    print(\"ðŸ§ª Creating test model...\")\n",
    "    test_model = build_model(test_cfg)\n",
    "    test_model.eval()\n",
    "    \n",
    "    # Test with clean data\n",
    "    if 'train_df_super_clean' in locals() and len(train_df_super_clean) > 0:\n",
    "        test_sample = train_df_super_clean.iloc[0]\n",
    "        test_input_ids = test_sample['input_ids'].unsqueeze(0)  # Add batch dimension\n",
    "        test_attention_mask = test_sample['attention_mask'].unsqueeze(0)\n",
    "        \n",
    "        print(f\"ðŸ”§ Testing forward pass...\")\n",
    "        print(f\"  Input shape: {test_input_ids.shape}\")\n",
    "        print(f\"  Input requires_grad: {test_input_ids.requires_grad}\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_output = test_model(test_input_ids, test_attention_mask)\n",
    "            print(f\"âœ… Forward pass successful!\")\n",
    "            print(f\"  Output logits shape: {test_output['logits'].shape}\")\n",
    "        \n",
    "        del test_model\n",
    "        print(f\"âœ… Gradient flow test passed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Gradient flow test failed: {str(e)}\")\n",
    "    print(f\"  Error type: {type(e).__name__}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a376010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Test - Run This First\n",
    "print(\"ðŸ§ª QUICK TEST CELL\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Test that everything works\n",
    "print(\"1. Testing basic Python...\")\n",
    "test_list = [1, 2, 3]\n",
    "print(f\"   âœ… List created: {test_list}\")\n",
    "\n",
    "print(\"2. Testing imports...\")\n",
    "try:\n",
    "    import torch\n",
    "    print(\"   âœ… PyTorch imported\")\n",
    "    print(f\"   âœ… CUDA available: {torch.cuda.is_available()}\")\n",
    "except Exception as e:\n",
    "    print(f\"   âŒ PyTorch import failed: {e}\")\n",
    "\n",
    "print(\"3. Testing variables...\")\n",
    "if 'train_df' in locals():\n",
    "    print(f\"   âœ… train_df exists with {len(train_df)} samples\")\n",
    "    print(f\"   âœ… Columns: {list(train_df.columns)}\")\n",
    "else:\n",
    "    print(\"   âŒ train_df not found\")\n",
    "\n",
    "if 'cfg' in locals():\n",
    "    print(f\"   âœ… cfg exists\")\n",
    "    print(f\"   âœ… cfg.max_epochs: {cfg.max_epochs}\")\n",
    "else:\n",
    "    print(\"   âŒ cfg not found\")\n",
    "\n",
    "print(\"4. Testing function imports...\")\n",
    "try:\n",
    "    from src.training.classifier_trainer import train_classifier, evaluate_from_dataframe\n",
    "    print(\"   âœ… Training functions imported\")\n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Function import failed: {e}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ If all tests pass, the validation cell should work!\")\n",
    "print(\"=\"*40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
