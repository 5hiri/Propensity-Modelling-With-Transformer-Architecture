{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "95962df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from src.utils.config import get_small_classifier_config\n",
    "from src.training.classifier_trainer import SimpleTextDataset, train_classifier, evaluate\n",
    "import csv, random, time, datetime as dt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.utils.char_tokenizer import CharTokenizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b81458",
   "metadata": {},
   "source": [
    "### Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f024419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "cfg = get_small_classifier_config()\n",
    "cfg.num_classes = 2  # binary\n",
    "\n",
    "# Adjust hyper parameters\n",
    "cfg.learning_rate = 1e-4\n",
    "cfg.weight_decay = 0.01\n",
    "cfg.max_epochs = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d973f902",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "43e69206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_pseudo_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>date_formatted</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>event_name</th>\n",
       "      <th>rev_usd</th>\n",
       "      <th>unique_items</th>\n",
       "      <th>qty</th>\n",
       "      <th>page_location</th>\n",
       "      <th>page_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u_001</td>\n",
       "      <td>s_001_01_w1</td>\n",
       "      <td>2025-08-25</td>\n",
       "      <td>1756036801000</td>\n",
       "      <td>session_start</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://example.com/</td>\n",
       "      <td>Home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u_001</td>\n",
       "      <td>s_001_01_w1</td>\n",
       "      <td>2025-08-25</td>\n",
       "      <td>1756036807000</td>\n",
       "      <td>page_view</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://example.com/category/equipment</td>\n",
       "      <td>Gym Equipment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u_001</td>\n",
       "      <td>s_001_01_w1</td>\n",
       "      <td>2025-08-25</td>\n",
       "      <td>1756036813000</td>\n",
       "      <td>view_item</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>https://example.com/product/sku1001</td>\n",
       "      <td>Foam Roller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u_001</td>\n",
       "      <td>s_001_01_w1</td>\n",
       "      <td>2025-08-25</td>\n",
       "      <td>1756036819000</td>\n",
       "      <td>add_to_cart</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>https://example.com/product/sku1001</td>\n",
       "      <td>Foam Roller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u_001</td>\n",
       "      <td>s_001_01_w1</td>\n",
       "      <td>2025-08-25</td>\n",
       "      <td>1756036825000</td>\n",
       "      <td>begin_checkout</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>https://example.com/cart</td>\n",
       "      <td>Cart</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_pseudo_id   session_id date_formatted  event_timestamp      event_name  \\\n",
       "0          u_001  s_001_01_w1     2025-08-25    1756036801000   session_start   \n",
       "1          u_001  s_001_01_w1     2025-08-25    1756036807000       page_view   \n",
       "2          u_001  s_001_01_w1     2025-08-25    1756036813000       view_item   \n",
       "3          u_001  s_001_01_w1     2025-08-25    1756036819000     add_to_cart   \n",
       "4          u_001  s_001_01_w1     2025-08-25    1756036825000  begin_checkout   \n",
       "\n",
       "   rev_usd  unique_items  qty                           page_location  \\\n",
       "0      0.0             0    0                    https://example.com/   \n",
       "1      0.0             0    0  https://example.com/category/equipment   \n",
       "2      0.0             1    1     https://example.com/product/sku1001   \n",
       "3      0.0             1    1     https://example.com/product/sku1001   \n",
       "4      0.0             1    1                https://example.com/cart   \n",
       "\n",
       "      page_title  \n",
       "0           Home  \n",
       "1  Gym Equipment  \n",
       "2    Foam Roller  \n",
       "3    Foam Roller  \n",
       "4           Cart  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_pseudo_id      object\n",
      "session_id          object\n",
      "date_formatted      object\n",
      "event_timestamp      int64\n",
      "event_name          object\n",
      "rev_usd            float64\n",
      "unique_items         int64\n",
      "qty                  int64\n",
      "page_location       object\n",
      "page_title          object\n",
      "dtype: object\n",
      "Rows: 900\n"
     ]
    }
   ],
   "source": [
    "csv_path = Path(\"fake-6-weeks.csv\")  # adjust if stored elsewhere\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# ensure numeric types\n",
    "df[\"rev_usd\"] = df[\"rev_usd\"].astype(float)\n",
    "df[\"event_timestamp\"] = df[\"event_timestamp\"].astype(\"int64\")\n",
    "\n",
    "display(df.head())\n",
    "print(df.dtypes)\n",
    "print(f\"Rows: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "03e8e944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u_001' 'u_002' 'u_003' 'u_004' 'u_005' 'u_006' 'u_007' 'u_008' 'u_009'\n",
      " 'u_010' 'u_011' 'u_012' 'u_013' 'u_014' 'u_015' 'u_016' 'u_017' 'u_018'\n",
      " 'u_019' 'u_020' 'u_021' 'u_022' 'u_023' 'u_024' 'u_025']\n",
      "Training Data Len: 600\n",
      "Distribution Balance: Counter({1: 450, 0: 150})\n",
      "Session-s_001_01_w1\n",
      "Date-2025-08-25\n",
      "evt: session_start, tm: 00:00, rev: $0.0, uq_itms: 0, qty: 0, loc: https://example.com/, title: Home\n",
      "evt: page_view, tm: 00:00, rev: $0.0, uq_itms: 0, qty: 0, loc: https://example.com/category/equipment, title: Gym Equipment\n",
      "evt: view_item, tm: 00:00, rev: $0.0, uq_itms: 1, qty: 1, loc: https://example.com/product/sku1001, title: Foam Roller\n",
      "evt: add_to_cart, tm: 00:00, rev: $0.0, uq_itms: 1, qty: 1, loc: https://example.com/product/sku1001, title: Foam Roller\n",
      "evt: begin_checkout, tm: 00:00, rev: $0.0, uq_itms: 1, qty: 1, loc: https://example.com/cart, title: Cart\n",
      "evt: purchase, tm: 00:00, rev: $29.49, uq_itms: 1, qty: 1, loc: https://example.com/checkout/complete, title: Order Confirmation\n",
      "Label: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      [48, 8, 22, 22, 12, 18, 84, 73, 22, 1, 56, 56,...\n",
       "1      [48, 8, 22, 22, 12, 18, 84, 73, 22, 1, 56, 56,...\n",
       "2      [48, 8, 22, 22, 12, 18, 84, 73, 22, 1, 56, 56,...\n",
       "3      [48, 8, 22, 22, 12, 18, 84, 73, 22, 1, 56, 56,...\n",
       "4      [48, 8, 22, 22, 12, 18, 84, 73, 22, 1, 56, 56,...\n",
       "                             ...                        \n",
       "595    [48, 8, 22, 22, 12, 18, 84, 73, 22, 1, 56, 58,...\n",
       "596    [48, 8, 22, 22, 12, 18, 84, 73, 22, 1, 56, 58,...\n",
       "597    [48, 8, 22, 22, 12, 18, 84, 73, 22, 1, 56, 58,...\n",
       "598    [48, 8, 22, 22, 12, 18, 84, 73, 22, 1, 56, 58,...\n",
       "599    [48, 8, 22, 22, 12, 18, 84, 73, 22, 1, 56, 58,...\n",
       "Name: text, Length: 600, dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data = df.copy()\n",
    "\n",
    "# Convert date_formatted to date time\n",
    "processed_data[\"date_formatted\"] = pd.to_datetime(processed_data[\"date_formatted\"])\n",
    "\n",
    "# grab unique user ids\n",
    "unique_user_ids = df[\"user_pseudo_id\"].unique()\n",
    "train_data = []\n",
    "print(unique_user_ids)\n",
    "\n",
    "for user_id in unique_user_ids:\n",
    "    user_data = processed_data[processed_data[\"user_pseudo_id\"] == user_id]\n",
    "\n",
    "    # Count unique mondays\n",
    "    monday_count = user_data[user_data[\"date_formatted\"].dt.dayofweek == 0][\"date_formatted\"].nunique()\n",
    "\n",
    "    event_len = len(user_data)\n",
    "    # print(f\"User ID: {user_id}, Number of Events: {event_len}, Number of Mondays: {monday_count}\")\n",
    "    if event_len < 6:\n",
    "        continue  # skip users with less than 10 events\n",
    "    for i in range(6, event_len):\n",
    "        main_event = user_data.iloc[i]\n",
    "        # Get start of main_week(monday)\n",
    "        main_start_of_week = main_event[\"date_formatted\"] - pd.to_timedelta(main_event[\"date_formatted\"].dayofweek, unit='d')\n",
    "        main_end_of_week = main_start_of_week + pd.DateOffset(days=6)\n",
    "        pred_start_of_week = main_end_of_week + pd.Timedelta(days=1)\n",
    "        pred_end_of_week = pred_start_of_week + pd.DateOffset(days=6)\n",
    "        # check if there is any data for next week to label\n",
    "        if user_data[user_data[\"date_formatted\"].between(pred_start_of_week, pred_end_of_week)].shape[0] == 0:\n",
    "            continue  # skip if no data for next week\n",
    "\n",
    "        context_events = user_data.iloc[:i]\n",
    "\n",
    "        # Get tagged prediction: if purchase event occurs in the following week return 1, else 0\n",
    "        get_tagged_prediction = 1 if user_data[user_data[\"date_formatted\"].between(pred_start_of_week, pred_end_of_week) & (user_data[\"event_name\"] == \"purchase\")].shape[0] > 0 else 0\n",
    "\n",
    "        # Group context events by session using session_id, not using groupby\n",
    "        context_sessions = []\n",
    "        for session_id, group in context_events.groupby(\"session_id\"):\n",
    "            context_sessions.append({\n",
    "                \"session_id\": session_id,\n",
    "                \"events\": group[[\"event_name\", \"date_formatted\", \"event_timestamp\", \"rev_usd\", \"unique_items\", \"qty\", \"page_location\", \"page_title\"]].values.tolist()\n",
    "            })\n",
    "\n",
    "        # Sort sessions by oldest to earliest date\n",
    "        context_sessions = sorted(context_sessions, key=lambda x: x[\"events\"][0][1])\n",
    "\n",
    "        train_data_record = \"\"\n",
    "        current_session_date = context_sessions[0][\"events\"][0][1]\n",
    "        first_session = True\n",
    "        for session in context_sessions:\n",
    "            if session[\"events\"][0][1] > current_session_date:\n",
    "                current_session_date = session[\"events\"][0][1]\n",
    "            if first_session:\n",
    "                train_data_record += f\"Session-{session['session_id']}\"\n",
    "            else:\n",
    "                train_data_record += f\"\\n\\nSession-{session['session_id']}\"\n",
    "            train_data_record += f\"\\nDate-{current_session_date.strftime('%Y-%m-%d')}\"\n",
    "            for event in session[\"events\"]:\n",
    "                if event[1] > current_session_date:\n",
    "                    current_session_date = event[1]\n",
    "                    train_data_record += f\"\\nDate-{current_session_date.strftime('%Y-%m-%d')}\"\n",
    "                # Convert event_timestamp (milliseconds) to HH:MM\n",
    "                event_time = dt.datetime.fromtimestamp(event[2] / 1000).strftime('%H:%M')\n",
    "                train_data_record += f\"\\nevt: {event[0]}, tm: {event_time}, rev: ${event[3]}, uq_itms: {event[4]}, qty: {event[5]}, loc: {event[6]}, title: {event[7]}\"\n",
    "\n",
    "        train_data.append({\n",
    "            \"text\": train_data_record,\n",
    "            \"label\": get_tagged_prediction\n",
    "        })\n",
    "\n",
    "print(f\"Training Data Len: {len(train_data)}\")\n",
    "print(f\"Distribution Balance: {Counter([d['label'] for d in train_data])}\")\n",
    "print(train_data[0][\"text\"])\n",
    "print(f\"Label: {train_data[0]['label']}\")\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "\n",
    "texts = train_df[\"text\"].tolist()\n",
    "\n",
    "char_tok = CharTokenizer()\n",
    "\n",
    "cfg.vocab_size = char_tok.vocab_size\n",
    "\n",
    "train_df[\"text\"].apply(lambda x: char_tok.encode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1134e4",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0e068f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = train_test_split(train_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc0a66d",
   "metadata": {},
   "source": [
    "### Train and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "330c1923",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "129",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\64219\\Documents\\GitHub\\Propensity-Modelling-With-Transformer-Architecture\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 129",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mtrain_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m torch.save(model.state_dict(), \u001b[33m\"\u001b[39m\u001b[33mclassifier_model.pt\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\64219\\Documents\\GitHub\\Propensity-Modelling-With-Transformer-Architecture\\src\\training\\classifier_trainer.py:111\u001b[39m, in \u001b[36mtrain_classifier\u001b[39m\u001b[34m(cfg, train_dataset, val_dataset, device)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg.max_epochs):\n\u001b[32m    110\u001b[39m     model.train()\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\64219\\Documents\\GitHub\\Propensity-Modelling-With-Transformer-Architecture\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\64219\\Documents\\GitHub\\Propensity-Modelling-With-Transformer-Architecture\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\64219\\Documents\\GitHub\\Propensity-Modelling-With-Transformer-Architecture\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\64219\\Documents\\GitHub\\Propensity-Modelling-With-Transformer-Architecture\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\64219\\Documents\\GitHub\\Propensity-Modelling-With-Transformer-Architecture\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 129"
     ]
    }
   ],
   "source": [
    "model = train_classifier(cfg, train_ds, val_ds)\n",
    "torch.save(model.state_dict(), \"classifier_model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
